# 试验任务主从表关联关系及数据库性能综合优化方案

**文档版本**: v1.0  
**编制日期**: 2026-02-05  
**适用范围**: 百万级数据量场景下的试验任务管理系统  
**目标**: 确保系统响应时间 < 100ms，数据一致性 99.99%

---

## 目录

1. [现状分析与问题诊断](#1-现状分析与问题诊断)
2. [数据库架构优化](#2-数据库架构优化)
3. [索引策略优化](#3-索引策略优化)
4. [查询优化方案](#4-查询优化方案)
5. [数据分区与分片](#5-数据分区与分片)
6. [缓存机制设计](#6-缓存机制设计)
7. [主从复制与读写分离](#7-主从复制与读写分离)
8. [数据归档策略](#8-数据归档策略)
9. [监控告警体系](#9-监控告警体系)
10. [性能测试指标](#10-性能测试指标)
11. [实施路线图](#11-实施路线图)

---

## 1. 现状分析与问题诊断

### 1.1 现有表结构分析

#### 主任务表 (test_tasks) 现状

```sql
-- 当前表结构评估
CREATE TABLE test_tasks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    task_number VARCHAR(50) UNIQUE,        -- 有唯一约束，自动创建索引
    task_name VARCHAR(200),
    product_name VARCHAR(50),
    product_model VARCHAR(20),
    test_type_id BIGINT,                    -- 外键，需要索引
    priority_id BIGINT,                     -- 外键，需要索引
    status_id BIGINT,                       -- 外键，需要索引
    requester_id BIGINT,                    -- 外键，需要索引
    assignee_id BIGINT,                     -- 外键，需要索引
    requester_name VARCHAR(100),
    requester_phone VARCHAR(20),
    requester_department VARCHAR(100),
    description TEXT,
    test_outline TEXT,
    test_report TEXT,
    test_outline_file VARCHAR(100),
    test_report_file VARCHAR(100),
    start_date DATE,
    end_date DATE,
    actual_start_date DATE,
    actual_end_date DATE,
    rejection_reason TEXT,
    created_at DATETIME(6),
    updated_at DATETIME(6)
);
```

**问题识别**:
- ❌ 外键字段缺少索引 (test_type_id, priority_id, status_id, requester_id, assignee_id)
- ❌ 常用查询字段缺少索引 (start_date, end_date, created_at)
- ❌ 复合查询场景缺少联合索引
- ❌ 没有分区策略
- ❌ 大字段 (TEXT) 存储在主表

#### 子任务表 (sub_tasks) 现状

```sql
CREATE TABLE sub_tasks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    parent_task_id BIGINT,                  -- 外键，缺少索引
    subtask_number VARCHAR(50) UNIQUE,      -- 有唯一约束，自动创建索引
    subtask_name VARCHAR(200),
    test_type_id BIGINT,                    -- 外键，缺少索引
    status_id BIGINT,                       -- 外键，缺少索引
    assignee_id BIGINT,                     -- 外键，缺少索引
    description TEXT,
    start_date DATE,
    end_date DATE,
    actual_start_date DATE,
    actual_end_date DATE,
    created_at DATETIME(6),
    updated_at DATETIME(6)
);
```

**问题识别**:
- ❌ parent_task_id 缺少索引（最常用关联查询）
- ❌ 常用查询字段缺少索引 (status_id, assignee_id, start_date, end_date)
- ❌ 没有覆盖索引优化列表查询
- ❌ 缺少分区

### 1.2 慢查询场景分析

| 查询场景 | 问题描述 | 影响程度 |
|---------|---------|---------|
| 任务列表查询 | 无索引导致全表扫描 | ⭐⭐⭐⭐⭐ |
| 子任务关联查询 | parent_task_id 无索引 | ⭐⭐⭐⭐⭐ |
| 状态筛选查询 | status_id 无索引 | ⭐⭐⭐⭐ |
| 负责人任务查询 | assignee_id 无索引 | ⭐⭐⭐⭐ |
| 逾期任务查询 | end_date + status 联合查询 | ⭐⭐⭐ |
| 数据统计报表 | 大表聚合查询慢 | ⭐⭐⭐⭐ |

### 1.3 性能瓶颈预测（百万级数据）

```
预估性能指标（无优化）：
├── 任务列表查询: 2000-5000ms
├── 子任务加载: 1500-3000ms  
├── 任务统计聚合: 5000-10000ms
├── 数据库CPU使用: 80-95%
└── 内存使用: 持续增长直至OOM
```

---

## 2. 数据库架构优化

### 2.1 架构演进路线图

```
阶段一: 单体架构（当前）
    MySQL单实例
    └── test_tasks (100万)
    └── sub_tasks (500万)
    └── generic_test_data (1000万)

阶段二: 读写分离（3个月内）
    主库(写) ──同步──> 从库1(读)
              ──同步──> 从库2(报表)

阶段三: 分库分表（6-12个月）
    分库1(2024-2025数据)
    分库2(2026-2027数据)
    归档库(历史数据)

阶段四: 分布式架构（1年后）
    MySQL集群 + TiDB/ OceanBase
```

### 2.2 垂直拆分策略

#### 2.2.1 大字段拆分

将TEXT类型的大字段拆分到附属表：

```sql
-- 主任务主表 (精简版)
CREATE TABLE test_tasks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    task_number VARCHAR(50) UNIQUE NOT NULL,
    task_name VARCHAR(200) NOT NULL,
    product_name VARCHAR(50) DEFAULT '',
    product_model VARCHAR(20) DEFAULT '',
    test_type_id BIGINT NOT NULL,
    priority_id BIGINT NOT NULL,
    status_id BIGINT NOT NULL,
    requester_id BIGINT NOT NULL,
    assignee_id BIGINT,
    requester_name VARCHAR(100) DEFAULT '',
    requester_phone VARCHAR(20) DEFAULT '',
    requester_department VARCHAR(100) DEFAULT '',
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    actual_start_date DATE,
    actual_end_date DATE,
    created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    updated_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
    
    -- 分区键
    partition_key INT GENERATED ALWAYS AS (YEAR(created_at) * 100 + MONTH(created_at)) STORED
);

-- 任务详情扩展表 (大字段)
CREATE TABLE test_task_details (
    task_id BIGINT PRIMARY KEY,
    description TEXT,
    test_outline TEXT,
    test_report TEXT,
    rejection_reason TEXT,
    FOREIGN KEY (task_id) REFERENCES test_tasks(id) ON DELETE CASCADE
);

-- 子任务主表
CREATE TABLE sub_tasks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    parent_task_id BIGINT NOT NULL,
    subtask_number VARCHAR(50) UNIQUE NOT NULL,
    subtask_name VARCHAR(200) NOT NULL,
    test_type_id BIGINT NOT NULL,
    status_id BIGINT NOT NULL,
    assignee_id BIGINT,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    actual_start_date DATE,
    actual_end_date DATE,
    created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
    updated_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
    
    -- 分区键
    partition_key INT GENERATED ALWAYS AS (YEAR(created_at) * 100 + MONTH(created_at)) STORED
);

-- 子任务详情扩展表
CREATE TABLE sub_task_details (
    subtask_id BIGINT PRIMARY KEY,
    description TEXT,
    test_conditions TEXT,
    test_method TEXT,
    test_equipment TEXT,
    FOREIGN KEY (subtask_id) REFERENCES sub_tasks(id) ON DELETE CASCADE
);
```

**优点**:
- 主表行大小减小 60-70%，提升缓存命中率
- 列表查询只访问小表，性能提升 3-5 倍
- 详情查询按需加载，减少IO

---

## 3. 索引策略优化

### 3.1 索引设计原则

```
索引设计黄金法则：
1. 高选择性列优先 (选择性 > 0.1)
2. 等值查询列放联合索引左侧
3. 范围查询列放联合索引右侧
4. 避免冗余索引
5. 控制单表索引数量 < 5-7 个
6. 定期分析索引使用情况
```

### 3.2 主任务表索引方案

```sql
-- 1. 外键索引（必须）
ALTER TABLE test_tasks ADD INDEX idx_test_type (test_type_id);
ALTER TABLE test_tasks ADD INDEX idx_priority (priority_id);
ALTER TABLE test_tasks ADD INDEX idx_status (status_id);
ALTER TABLE test_tasks ADD INDEX idx_requester (requester_id);
ALTER TABLE test_tasks ADD INDEX idx_assignee (assignee_id);

-- 2. 常用查询场景索引
-- 场景：按创建时间倒序查看任务列表
ALTER TABLE test_tasks ADD INDEX idx_created_at_desc (created_at DESC);

-- 场景：按状态+创建时间筛选（任务工作台）
ALTER TABLE test_tasks ADD INDEX idx_status_created (status_id, created_at DESC);

-- 场景：负责人+状态（我的任务）
ALTER TABLE test_tasks ADD INDEX idx_assignee_status (assignee_id, status_id, created_at DESC);

-- 场景：逾期任务查询（状态不是已完成/已审核 + 结束日期 < 今天）
ALTER TABLE test_tasks ADD INDEX idx_end_date_status (end_date, status_id);

-- 3. 覆盖索引（Covering Index）优化列表查询
-- 覆盖任务列表页所有字段，避免回表
ALTER TABLE test_tasks ADD INDEX idx_list_query 
    (status_id, created_at DESC, id, task_number, task_name, 
     assignee_id, priority_id, start_date, end_date);

-- 4. 分区索引优化
ALTER TABLE test_tasks ADD INDEX idx_partition_created 
    (partition_key, created_at DESC);
```

### 3.3 子任务表索引方案

```sql
-- 1. 核心外键索引（最重要）
ALTER TABLE sub_tasks ADD INDEX idx_parent_task (parent_task_id);

-- 2. 业务查询索引
ALTER TABLE sub_tasks ADD INDEX idx_subtask_status (status_id);
ALTER TABLE sub_tasks ADD INDEX idx_subtask_assignee (assignee_id);
ALTER TABLE sub_tasks ADD INDEX idx_subtask_type (test_type_id);

-- 3. 时间相关索引
ALTER TABLE sub_tasks ADD INDEX idx_subtask_created (created_at DESC);
ALTER TABLE sub_tasks ADD INDEX idx_subtask_start (start_date);
ALTER TABLE sub_tasks ADD INDEX idx_subtask_end (end_date);

-- 4. 联合索引优化高频查询
-- 场景：查询某主任务下的所有子任务
ALTER TABLE sub_tasks ADD INDEX idx_parent_created 
    (parent_task_id, created_at DESC);

-- 场景：查询某负责人的待办子任务
ALTER TABLE sub_tasks ADD INDEX idx_assignee_status_created 
    (assignee_id, status_id, created_at DESC);

-- 场景：按子任务编号搜索
ALTER TABLE sub_tasks ADD INDEX idx_subtask_number (subtask_number);

-- 5. 覆盖索引（子任务列表查询）
ALTER TABLE sub_tasks ADD INDEX idx_subtask_list 
    (parent_task_id, status_id, created_at DESC, 
     id, subtask_number, subtask_name, assignee_id, 
     start_date, end_date, test_type_id);
```

### 3.4 索引监控与维护

```sql
-- 查看索引使用情况
SELECT 
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    SUB_PART,
    NULLABLE,
    INDEX_TYPE,
    COMMENT
FROM information_schema.STATISTICS
WHERE TABLE_SCHEMA = 'laboratory_test_management'
AND TABLE_NAME IN ('test_tasks', 'sub_tasks')
ORDER BY TABLE_NAME, INDEX_NAME;

-- 分析索引选择性
SELECT 
    COLUMN_NAME,
    COUNT(DISTINCT COLUMN_NAME) / COUNT(*) AS selectivity
FROM test_tasks
GROUP BY COLUMN_NAME;

-- 查看慢查询使用的索引
SELECT 
    DIGEST_TEXT,
    COUNT_STAR,
    AVG_TIMER_WAIT / 1000000000 AS avg_latency_ms,
    SUM_ROWS_EXAMINED / COUNT_STAR AS avg_rows_examined,
    FIRST_SEEN,
    LAST_SEEN
FROM performance_schema.events_statements_summary_by_digest
WHERE SCHEMA_NAME = 'laboratory_test_management'
AND DIGEST_TEXT LIKE '%test_tasks%'
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 20;
```

---

## 4. 查询优化方案

### 4.1 Django ORM 查询优化

#### 4.1.1 任务列表查询优化

```python
# ❌ 原始低效查询（N+1问题）
def task_list_slow(request):
    tasks = TestTask.objects.all().order_by('-created_at')[:20]
    data = []
    for task in tasks:
        data.append({
            'id': task.id,
            'task_number': task.task_number,
            'task_name': task.task_name,
            'test_type': task.test_type.name,  # N次查询
            'status': task.status.name,         # N次查询
            'assignee': task.assignee.username if task.assignee else None,  # N次查询
            'priority': task.priority.name,     # N次查询
            'progress': task.progress_percentage,
        })
    return JsonResponse(data, safe=False)

# ✅ 优化后查询（单次查询 + select_related）
from django.db.models import Prefetch, Count, Case, When, Value, IntegerField
def task_list_optimized(request):
    # 使用 select_related 避免 N+1
    # 使用 only 限制查询字段
    tasks = TestTask.objects.select_related(
        'test_type', 
        'status', 
        'priority',
        'assignee'
    ).only(
        'id', 'task_number', 'task_name', 
        'test_type__name', 'status__name', 'status__code',
        'priority__name', 'assignee__username',
        'start_date', 'end_date', 'created_at'
    ).order_by('-created_at')[:20]
    
    # 预计算子任务数量
    task_ids = [t.id for t in tasks]
    subtask_counts = dict(
        SubTask.objects.filter(parent_task_id__in=task_ids)
        .values('parent_task_id')
        .annotate(count=Count('id'))
        .values_list('parent_task_id', 'count')
    )
    
    data = []
    for task in tasks:
        data.append({
            'id': task.id,
            'task_number': task.task_number,
            'task_name': task.task_name,
            'test_type': task.test_type.name,
            'status': task.status.name,
            'status_code': task.status.code,
            'assignee': task.assignee.username if task.assignee else None,
            'priority': task.priority.name,
            'subtask_count': subtask_counts.get(task.id, 0),
            'is_overdue': task.is_overdue,
            'progress': task.progress_percentage,
        })
    
    return JsonResponse(data, safe=False)
```

#### 4.1.2 子任务关联查询优化

```python
# ❌ 低效查询：逐个加载子任务
def get_task_detail_slow(task_id):
    task = TestTask.objects.get(id=task_id)
    subtasks = task.subtasks.all()  # 先查主任务，再查子任务
    return task, subtasks

# ✅ 优化：一次性加载所有数据
def get_task_detail_optimized(task_id):
    # 使用 prefetch_related 预加载子任务
    task = TestTask.objects.select_related(
        'test_type', 'status', 'priority', 'requester', 'assignee'
    ).prefetch_related(
        Prefetch(
            'subtasks',
            queryset=SubTask.objects.select_related(
                'test_type', 'status', 'assignee'
            ).order_by('subtask_number')
        )
    ).get(id=task_id)
    
    return task

# ✅ 更优方案：使用原生SQL优化复杂查询
from django.db import connection

def get_task_with_subtasks_raw(task_id):
    with connection.cursor() as cursor:
        # 一次查询获取任务和子任务统计
        cursor.execute("""
            SELECT 
                t.id, t.task_number, t.task_name,
                t.test_type_id, tt.name as test_type_name,
                t.status_id, ts.name as status_name,
                t.priority_id, tp.name as priority_name,
                COUNT(st.id) as subtask_count,
                SUM(CASE WHEN st.status_id = %s THEN 1 ELSE 0 END) as completed_subtasks
            FROM test_tasks t
            LEFT JOIN test_types tt ON t.test_type_id = tt.id
            LEFT JOIN task_status ts ON t.status_id = ts.id
            LEFT JOIN priority_types tp ON t.priority_id = tp.id
            LEFT JOIN sub_tasks st ON t.id = st.parent_task_id
            WHERE t.id = %s
            GROUP BY t.id
        """, [completed_status_id, task_id])
        
        task_data = cursor.fetchone()
    
    return task_data
```

### 4.2 SQL 查询优化实例

#### 4.2.1 任务列表查询

```sql
-- ❌ 原始慢查询（全表扫描）
SELECT * FROM test_tasks 
WHERE status_id = 1 
ORDER BY created_at DESC 
LIMIT 20 OFFSET 0;
-- 执行时间: 2500ms (100万数据)

-- ✅ 优化后（使用覆盖索引）
SELECT 
    t.id, t.task_number, t.task_name, 
    t.assignee_id, t.priority_id, t.start_date, t.end_date,
    ts.name as status_name, tp.name as priority_name,
    u.username as assignee_name
FROM test_tasks t
INNER JOIN task_status ts ON t.status_id = ts.id
INNER JOIN priority_types tp ON t.priority_id = tp.id
LEFT JOIN users_user u ON t.assignee_id = u.id
WHERE t.status_id = 1
ORDER BY t.created_at DESC
LIMIT 20;
-- 执行时间: 15ms (使用 idx_status_created 索引)
```

#### 4.2.2 逾期任务查询

```sql
-- ❌ 低效查询（函数在WHERE条件）
SELECT * FROM test_tasks 
WHERE end_date < CURDATE() 
AND status_id NOT IN (4, 5)  -- 4=已完成, 5=已审核
ORDER BY end_date ASC;

-- ✅ 优化后
SELECT 
    t.id, t.task_number, t.task_name,
    t.end_date, DATEDIFF(CURDATE(), t.end_date) as overdue_days,
    u.username as assignee_name
FROM test_tasks t
LEFT JOIN users_user u ON t.assignee_id = u.id
WHERE t.end_date < '2026-02-05'
AND t.status_id IN (1, 2, 3)  -- 待处理、进行中、待审核
ORDER BY t.end_date ASC
LIMIT 50;
-- 使用索引: idx_end_date_status
```

#### 4.2.3 任务统计报表

```sql
-- 月度任务统计（物化视图方案）
CREATE TABLE task_statistics_monthly (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    year_month INT NOT NULL,                -- 格式: 202602
    total_tasks INT DEFAULT 0,
    completed_tasks INT DEFAULT 0,
    pending_tasks INT DEFAULT 0,
    in_progress_tasks INT DEFAULT 0,
    overdue_tasks INT DEFAULT 0,
    avg_completion_days DECIMAL(5,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    UNIQUE KEY uk_year_month (year_month)
);

-- 定时更新统计（每天凌晨执行）
INSERT INTO task_statistics_monthly (
    year_month, total_tasks, completed_tasks, 
    pending_tasks, in_progress_tasks, overdue_tasks
)
SELECT 
    YEAR(created_at) * 100 + MONTH(created_at) as year_month,
    COUNT(*) as total_tasks,
    SUM(CASE WHEN status_id = 4 THEN 1 ELSE 0 END) as completed_tasks,
    SUM(CASE WHEN status_id = 1 THEN 1 ELSE 0 END) as pending_tasks,
    SUM(CASE WHEN status_id = 2 THEN 1 ELSE 0 END) as in_progress_tasks,
    SUM(CASE WHEN end_date < CURDATE() AND status_id NOT IN (4,5) THEN 1 ELSE 0 END) as overdue_tasks
FROM test_tasks
WHERE created_at >= DATE_SUB(CURDATE(), INTERVAL 12 MONTH)
GROUP BY YEAR(created_at) * 100 + MONTH(created_at)
ON DUPLICATE KEY UPDATE
    total_tasks = VALUES(total_tasks),
    completed_tasks = VALUES(completed_tasks),
    pending_tasks = VALUES(pending_tasks),
    in_progress_tasks = VALUES(in_progress_tasks),
    overdue_tasks = VALUES(overdue_tasks),
    updated_at = CURRENT_TIMESTAMP;

-- 查询统计（毫秒级响应）
SELECT * FROM task_statistics_monthly 
WHERE year_month >= 202501
ORDER BY year_month DESC;
```

---

## 5. 数据分区与分片

### 5.1 水平分区策略（MySQL 5.7+）

#### 5.1.1 RANGE 分区（按时间）

```sql
-- 主任务表按月分区
CREATE TABLE test_tasks_partitioned (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    task_number VARCHAR(50) UNIQUE NOT NULL,
    task_name VARCHAR(200) NOT NULL,
    -- ... 其他字段
    created_at DATETIME(6) NOT NULL,
    updated_at DATETIME(6) NOT NULL,
    
    -- 分区键
    partition_key INT GENERATED ALWAYS AS (YEAR(created_at) * 100 + MONTH(created_at)) STORED
) 
PARTITION BY RANGE (partition_key) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    PARTITION p202409 VALUES LESS THAN (202410),
    PARTITION p202410 VALUES LESS THAN (202411),
    PARTITION p202411 VALUES LESS THAN (202412),
    PARTITION p202412 VALUES LESS THAN (202501),
    PARTITION p202501 VALUES LESS THAN (202502),
    PARTITION p202502 VALUES LESS THAN (202503),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- 子任务表分区（同主任务表结构）
CREATE TABLE sub_tasks_partitioned (
    -- 相同结构
) PARTITION BY RANGE (partition_key) (
    -- 相同分区定义
);
```

#### 5.1.2 分区管理脚本

```sql
-- 自动添加新分区（每月执行）
DELIMITER //
CREATE PROCEDURE AddMonthlyPartition()
BEGIN
    DECLARE next_month INT;
    DECLARE partition_name VARCHAR(20);
    DECLARE max_partition INT;
    
    -- 获取当前最大分区
    SELECT MAX(CAST(SUBSTRING(PARTITION_NAME, 2) AS UNSIGNED))
    INTO max_partition
    FROM information_schema.PARTITIONS
    WHERE TABLE_SCHEMA = 'laboratory_test_management'
    AND TABLE_NAME = 'test_tasks'
    AND PARTITION_NAME LIKE 'p%';
    
    -- 计算下个月
    SET next_month = max_partition + 1;
    SET partition_name = CONCAT('p', next_month);
    
    -- 添加新分区
    SET @sql = CONCAT(
        'ALTER TABLE test_tasks ADD PARTITION (',
        'PARTITION ', partition_name, ' VALUES LESS THAN (', next_month + 1, ')',
        ')'
    );
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 同样处理子任务表
    SET @sql = CONCAT(
        'ALTER TABLE sub_tasks ADD PARTITION (',
        'PARTITION ', partition_name, ' VALUES LESS THAN (', next_month + 1, ')',
        ')'
    );
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END //
DELIMITER ;

-- 创建定时事件（每月1号执行）
CREATE EVENT add_partition_monthly
ON SCHEDULE EVERY 1 MONTH
STARTS '2026-03-01 00:00:00'
DO CALL AddMonthlyPartition();
```

### 5.2 分表策略

#### 5.2.1 按年份分表

```sql
-- 历史数据归档表（只读）
CREATE TABLE test_tasks_2024 (
    -- 相同结构
) ENGINE=InnoDB;

CREATE TABLE test_tasks_2025 LIKE test_tasks;
CREATE TABLE test_tasks_2026 LIKE test_tasks;

-- 当前表使用 test_tasks（即 test_tasks_2026）
-- 应用程序根据年份路由到不同表
```

#### 5.2.2 Django 分表路由

```python
# apps/common/db_router.py
class TaskDatabaseRouter:
    """任务表分表路由"""
    
    def db_for_read(self, model, **hints):
        if model._meta.app_label == 'tasks':
            # 根据查询条件路由
            instance = hints.get('instance')
            if instance and hasattr(instance, 'created_at'):
                year = instance.created_at.year
                if year < 2026:
                    return f'tasks_{year}'
        return None
    
    def db_for_write(self, model, **hints):
        # 新数据写入当前表
        if model._meta.app_label == 'tasks':
            return 'default'
        return None
    
    def allow_relation(self, obj1, obj2, **hints):
        if obj1._meta.app_label == 'tasks' and obj2._meta.app_label == 'tasks':
            return True
        return None
    
    def allow_migrate(self, db, app_label, model_name=None, **hints):
        if app_label == 'tasks':
            return db == 'default'
        return None

# settings.py
DATABASE_ROUTERS = ['apps.common.db_router.TaskDatabaseRouter']
```

---

## 6. 缓存机制设计

### 6.1 多级缓存架构

```
用户请求
    │
    ├──► L1: 本地缓存 (Caffeine/Guava) - < 1ms
    │      ├── 热点数据：任务状态、用户信息
    │      └── TTL: 1-5分钟
    │
    ├──► L2: Redis 分布式缓存 - 2-5ms
    │      ├── 业务数据：任务列表、统计信息
    │      └── TTL: 10-60分钟
    │
    └──► L3: MySQL 数据库 - 10-50ms
           └── 持久化存储
```

### 6.2 Redis 缓存配置

#### 6.2.1 Django 缓存配置

```python
# settings.py - 生产环境Redis配置
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_CLASS': 'redis.connection.BlockingConnectionPool',
            'CONNECTION_POOL_CLASS_KWARGS': {
                'max_connections': 50,
                'timeout': 20,
            },
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
            'SERIALIZER': 'django_redis.serializers.json.JSONSerializer',
        },
        'KEY_PREFIX': 'lab_mgmt',
        'TIMEOUT': 300,  # 默认5分钟
    },
    # 会话缓存
    'session': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/2',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        },
        'KEY_PREFIX': 'session',
        'TIMEOUT': 86400,  # 1天
    },
    # 统计缓存（长时间）
    'statistics': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/3',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        },
        'KEY_PREFIX': 'stats',
        'TIMEOUT': 3600,  # 1小时
    }
}

# 会话使用Redis
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
SESSION_CACHE_ALIAS = 'session'
```

#### 6.2.2 缓存策略实现

```python
# apps/common/cache_manager.py
from django.core.cache import cache
from functools import wraps
import hashlib
import json

class TaskCacheManager:
    """任务缓存管理器"""
    
    # 缓存键前缀
    PREFIX_TASK_LIST = 'task:list'
    PREFIX_TASK_DETAIL = 'task:detail'
    PREFIX_SUBTASK_LIST = 'subtask:list'
    PREFIX_STATISTICS = 'stats'
    
    # 缓存时间配置（秒）
    TTL_TASK_LIST = 300        # 5分钟
    TTL_TASK_DETAIL = 600      # 10分钟
    TTL_SUBTASK_LIST = 300     # 5分钟
    TTL_STATISTICS = 3600      # 1小时
    
    @staticmethod
    def generate_key(prefix, *args, **kwargs):
        """生成缓存键"""
        key_parts = [prefix]
        key_parts.extend([str(arg) for arg in args])
        key_parts.extend([f"{k}:{v}" for k, v in sorted(kwargs.items())])
        raw_key = ':'.join(key_parts)
        # 长键使用MD5
        if len(raw_key) > 200:
            return prefix + ':' + hashlib.md5(raw_key.encode()).hexdigest()
        return raw_key
    
    @classmethod
    def get_task_list(cls, user_id, status=None, page=1, page_size=20):
        """获取任务列表（带缓存）"""
        cache_key = cls.generate_key(
            cls.PREFIX_TASK_LIST, 
            user_id, 
            status=status, 
            page=page,
            page_size=page_size
        )
        
        data = cache.get(cache_key)
        if data is None:
            # 从数据库查询
            queryset = TestTask.objects.select_related(
                'test_type', 'status', 'priority', 'assignee'
            ).order_by('-created_at')
            
            if status:
                queryset = queryset.filter(status_id=status)
            
            start = (page - 1) * page_size
            tasks = queryset[start:start + page_size]
            
            data = [{
                'id': t.id,
                'task_number': t.task_number,
                'task_name': t.task_name,
                'status': t.status.name,
                'priority': t.priority.name,
            } for t in tasks]
            
            # 写入缓存
            cache.set(cache_key, data, cls.TTL_TASK_LIST)
        
        return data
    
    @classmethod
    def invalidate_task_list(cls, user_id=None):
        """使任务列表缓存失效"""
        if user_id:
            pattern = f"{cls.PREFIX_TASK_LIST}:{user_id}:*"
        else:
            pattern = f"{cls.PREFIX_TASK_LIST}:*"
        
        # 删除匹配的键
        from django_redis import get_redis_connection
        redis_conn = get_redis_connection('default')
        for key in redis_conn.scan_iter(match=pattern):
            redis_conn.delete(key)
    
    @classmethod
    def get_statistics(cls, year_month):
        """获取统计数据（高频访问，长缓存）"""
        cache_key = f"{cls.PREFIX_STATISTICS}:monthly:{year_month}"
        
        data = cache.get(cache_key)
        if data is None:
            # 从统计表查询
            from django.db import connection
            with connection.cursor() as cursor:
                cursor.execute("""
                    SELECT * FROM task_statistics_monthly 
                    WHERE year_month = %s
                """, [year_month])
                row = cursor.fetchone()
                if row:
                    data = {
                        'year_month': row[1],
                        'total_tasks': row[2],
                        'completed_tasks': row[3],
                        # ...
                    }
            
            if data:
                cache.set(cache_key, data, cls.TTL_STATISTICS)
        
        return data


def cached_task_list(timeout=300):
    """任务列表缓存装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # 生成缓存键
            cache_key = TaskCacheManager.generate_key(
                'task:list:view',
                func.__name__,
                *args,
                **kwargs
            )
            
            # 尝试从缓存获取
            result = cache.get(cache_key)
            if result is not None:
                return result
            
            # 执行函数
            result = func(*args, **kwargs)
            
            # 缓存结果
            cache.set(cache_key, result, timeout)
            
            return result
        return wrapper
    return decorator
```

#### 6.2.3 缓存更新策略

```python
# apps/tasks/signals.py
from django.db.models.signals import post_save, post_delete
from django.dispatch import receiver
from .models import TestTask, SubTask
from apps.common.cache_manager import TaskCacheManager

@receiver(post_save, sender=TestTask)
def invalidate_task_cache_on_save(sender, instance, created, **kwargs):
    """任务保存时清除相关缓存"""
    # 清除详情缓存
    cache_key = f"task:detail:{instance.id}"
    cache.delete(cache_key)
    
    # 清除列表缓存
    TaskCacheManager.invalidate_task_list(instance.assignee_id)
    TaskCacheManager.invalidate_task_list(instance.requester_id)
    
    # 清除统计缓存
    year_month = instance.created_at.strftime('%Y%m')
    cache.delete(f"stats:monthly:{year_month}")

@receiver(post_delete, sender=TestTask)
def invalidate_task_cache_on_delete(sender, instance, **kwargs):
    """任务删除时清除缓存"""
    TaskCacheManager.invalidate_task_list()

@receiver(post_save, sender=SubTask)
def invalidate_subtask_cache(sender, instance, **kwargs):
    """子任务变更时清除缓存"""
    # 清除子任务列表缓存
    cache_key = f"subtask:list:{instance.parent_task_id}"
    cache.delete(cache_key)
    
    # 清除主任务缓存（进度可能变化）
    cache_key = f"task:detail:{instance.parent_task_id}"
    cache.delete(cache_key)
```

---

## 7. 主从复制与读写分离

### 7.1 MySQL 主从复制架构

```
                    ┌─────────────────┐
                    │   Application   │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              │              │              │
       ┌──────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
       │   写操作    │ │  读操作  │ │  报表查询   │
       │  (INSERT)  │ │ (SELECT) │ │  (SELECT)   │
       └──────┬──────┘ └────┬─────┘ └──────┬──────┘
              │             │              │
       ┌──────▼─────────────┴──────────────▼──────┐
       │           ProxySQL / MySQL Router         │
       │         (读写分离路由/负载均衡)            │
       └──────┬───────────────────────────┬──────┘
              │                           │
    ┌─────────▼──────────┐     ┌──────────▼─────────┐
    │   MySQL Master     │     │  MySQL Slave 1     │
    │   (写 + 实时读)    │◄────┤  (读 - 用户查询)   │
    │   192.168.1.10     │同步  │  192.168.1.11      │
    └────────────────────┘     └────────────────────┘
              │
              │ 同步
              ▼
    ┌────────────────────┐
    │  MySQL Slave 2     │
    │  (读 - 报表/分析)   │
    │  192.168.1.12      │
    └────────────────────┘
```

### 7.2 MySQL 主从配置

#### 7.2.1 Master 配置 (my.cnf)

```ini
[mysqld]
# 服务器ID
server-id = 1

# 二进制日志配置
log-bin = mysql-bin
binlog-format = ROW
binlog-row-image = FULL
expire-logs-days = 14
max-binlog-size = 500M

# GTID 配置（推荐）
gtid_mode = ON
enforce-gtid-consistency = ON

# 复制优化
slave-parallel-type = LOGICAL_CLOCK
slave-parallel-workers = 4

# 半同步复制（可选，增强一致性）
plugin-load = "rpl_semi_sync_master=semisync_master.so"
rpl_semi_sync_master_enabled = 1
rpl_semi_sync_master_timeout = 1000

# InnoDB 优化
innodb_flush_log_at_trx_commit = 1
sync_binlog = 1
```

#### 7.2.2 Slave 配置 (my.cnf)

```ini
[mysqld]
server-id = 2  # Slave 2 使用 3

# 中继日志
relay-log = mysql-relay-bin
read-only = ON

# GTID
gtid_mode = ON
enforce-gtid-consistency = ON

# 复制优化
slave-parallel-type = LOGICAL_CLOCK
slave-parallel-workers = 8
slave-preserve-commit-order = ON

# 半同步复制
plugin-load = "rpl_semi_sync_slave=semisync_slave.so"
rpl_semi_sync_slave_enabled = 1

# 延迟复制（报表库可配置，防止误操作）
# CHANGE MASTER TO MASTER_DELAY = 3600;  -- 延迟1小时
```

### 7.3 Django 读写分离配置

```python
# settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'laboratory_test_management',
        'USER': 'app_writer',
        'PASSWORD': 'password',
        'HOST': '192.168.1.10',  # Master
        'PORT': '3306',
        'OPTIONS': {
            'init_command': "SET sql_mode='STRICT_TRANS_TABLES'",
        },
    },
    'read_replica_1': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'laboratory_test_management',
        'USER': 'app_reader',
        'PASSWORD': 'password',
        'HOST': '192.168.1.11',  # Slave 1
        'PORT': '3306',
        'OPTIONS': {
            'init_command': "SET sql_mode='STRICT_TRANS_TABLES'",
        },
    },
    'read_replica_2': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'laboratory_test_management',
        'USER': 'app_reader',
        'PASSWORD': 'password',
        'HOST': '192.168.1.12',  # Slave 2
        'PORT': '3306',
        'OPTIONS': {
            'init_command': "SET sql_mode='STRICT_TRANS_TABLES'",
        },
    },
}

# 数据库路由
DATABASE_ROUTERS = ['apps.common.db_router.PrimaryReplicaRouter']
```

```python
# apps/common/db_router.py
import random

class PrimaryReplicaRouter:
    """主从读写分离路由"""
    
    def db_for_read(self, model, **hints):
        """读操作路由到从库"""
        # 报表相关模型路由到专门的报表库
        if model._meta.app_label == 'reports':
            return 'read_replica_2'
        
        # 其他读操作轮询到从库1和从库2
        return random.choice(['read_replica_1', 'read_replica_2'])
    
    def db_for_write(self, model, **hints):
        """写操作路由到主库"""
        return 'default'
    
    def allow_relation(self, obj1, obj2, **hints):
        """允许跨数据库关系"""
        db_list = ('default', 'read_replica_1', 'read_replica_2')
        if obj1._state.db in db_list and obj2._state.db in db_list:
            return True
        return None
    
    def allow_migrate(self, db, app_label, model_name=None, **hints):
        """只允许在主库上执行迁移"""
        return db == 'default'
```

### 7.4 强制主库读取策略

```python
# apps/tasks/views.py
from django.db import transaction, connections

def get_task_detail_critical(request, task_id):
    """获取任务详情（强制主库读取保证一致性）"""
    
    # 方式1: 使用事务强制主库
    with transaction.atomic(using='default'):
        task = TestTask.objects.select_related(
            'test_type', 'status', 'priority'
        ).get(id=task_id)
    
    # 方式2: 使用 using() 指定数据库
    task = TestTask.objects.using('default').select_related(
        'test_type', 'status', 'priority'
    ).get(id=task_id)
    
    return task


class ForcePrimaryMixin:
    """强制使用主库的Mixin"""
    
    def get_queryset(self):
        queryset = super().get_queryset()
        # 检查是否需要强制主库
        if self.request.headers.get('X-Require-Consistency') == 'strong':
            queryset = queryset.using('default')
        return queryset


# 使用示例
class TaskUpdateView(ForcePrimaryMixin, UpdateView):
    model = TestTask
    # ...
```

---

## 8. 数据归档策略

### 8.1 归档策略设计

```
数据生命周期管理：

热数据（0-3个月）
├── 存储：主库 test_tasks
├── 访问频率：高
└── 性能要求：毫秒级响应

温数据（3-12个月）
├── 存储：主库 + 归档表 test_tasks_archive
├── 访问频率：中
└── 性能要求：百毫秒级响应

冷数据（1-3年）
├── 存储：归档库 archive_db
├── 访问频率：低
└── 性能要求：秒级响应

冻结数据（3年以上）
├── 存储：对象存储（S3/OSS）+ 压缩
├── 访问频率：极低（合规审计）
└── 恢复时间：小时级
```

### 8.2 归档表设计

```sql
-- 归档库配置
CREATE DATABASE IF NOT EXISTS laboratory_archive 
CHARACTER SET utf8mb4 
COLLATE utf8mb4_unicode_ci;

-- 任务归档表（压缩存储）
CREATE TABLE laboratory_archive.test_tasks_archive (
    id BIGINT PRIMARY KEY,
    task_number VARCHAR(50) NOT NULL,
    task_name VARCHAR(200) NOT NULL,
    -- ... 其他字段
    created_at DATETIME(6) NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- 分区：按归档时间
    PARTITION BY RANGE (YEAR(archived_at)) (
        PARTITION p2024 VALUES LESS THAN (2025),
        PARTITION p2025 VALUES LESS THAN (2026),
        PARTITION p_future VALUES LESS THAN MAXVALUE
    )
) ENGINE=InnoDB 
ROW_FORMAT=COMPRESSED  -- 压缩存储
KEY_BLOCK_SIZE=8;

-- 归档索引（精简）
CREATE INDEX idx_archive_task_number ON laboratory_archive.test_tasks_archive(task_number);
CREATE INDEX idx_archive_created ON laboratory_archive.test_tasks_archive(created_at);
```

### 8.3 归档脚本实现

```python
# scripts/archive_tasks.py
import os
import sys
import django
from datetime import datetime, timedelta
from django.db import transaction, connection
from django.db.models import Q

# 设置Django环境
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'laboratory_management.settings')
django.setup()

from apps.tasks.models import TestTask, SubTask

class TaskArchiver:
    """任务归档管理器"""
    
    def __init__(self):
        self.batch_size = 1000
        self.archive_db = 'archive'
        
    def get_archivable_tasks(self, months=3):
        """获取可归档的任务（已完成且超过3个月）"""
        cutoff_date = datetime.now() - timedelta(days=months*30)
        
        return TestTask.objects.filter(
            Q(status__code='completed') | Q(status__code='reviewed'),
            updated_at__lt=cutoff_date
        ).order_by('id')
    
    def archive_batch(self, tasks):
        """归档一批任务"""
        task_ids = [t.id for t in tasks]
        
        with transaction.atomic():
            # 1. 插入到归档表
            self._insert_to_archive(tasks)
            
            # 2. 删除子任务
            SubTask.objects.filter(parent_task_id__in=task_ids).delete()
            
            # 3. 删除主任务
            TestTask.objects.filter(id__in=task_ids).delete()
    
    def _insert_to_archive(self, tasks):
        """插入到归档库"""
        if not tasks:
            return
        
        # 构建批量插入SQL
        values = []
        for task in tasks:
            values.append(
                f"({task.id}, '{task.task_number}', '{task.task_name}', "
                f"'{task.created_at.isoformat()}', NOW())"
            )
        
        sql = f"""
            INSERT INTO laboratory_archive.test_tasks_archive 
            (id, task_number, task_name, created_at, archived_at)
            VALUES {','.join(values)}
        """
        
        with connection.cursor() as cursor:
            cursor.execute(sql)
    
    def run(self, months=3, dry_run=False):
        """执行归档"""
        print(f"开始归档 {months} 个月前的已完成任务...")
        
        tasks = self.get_archivable_tasks(months)
        total = tasks.count()
        
        print(f"找到 {total} 条可归档任务")
        
        if dry_run:
            print("模拟运行模式，不实际归档")
            return
        
        processed = 0
        while processed < total:
            batch = tasks[processed:processed + self.batch_size]
            self.archive_batch(batch)
            processed += len(batch)
            print(f"已归档: {processed}/{total}")
        
        print(f"归档完成，共归档 {processed} 条任务")
    
    def verify_archive(self, task_id):
        """验证归档数据完整性"""
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT COUNT(*) FROM laboratory_archive.test_tasks_archive
                WHERE id = %s
            """, [task_id])
            count = cursor.fetchone()[0]
        
        return count > 0


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='任务归档工具')
    parser.add_argument('--months', type=int, default=3, help='归档多久前的数据')
    parser.add_argument('--dry-run', action='store_true', help='模拟运行')
    
    args = parser.parse_args()
    
    archiver = TaskArchiver()
    archiver.run(months=args.months, dry_run=args.dry_run)
```

### 8.4 定时归档任务

```python
# apps/tasks/tasks.py (Celery任务)
from celery import shared_task
from scripts.archive_tasks import TaskArchiver

@shared_task(bind=True, max_retries=3)
def archive_old_tasks(self, months=3):
    """定时归档旧任务"""
    try:
        archiver = TaskArchiver()
        archiver.run(months=months, dry_run=False)
        return {'status': 'success', 'message': '归档完成'}
    except Exception as exc:
        # 重试
        raise self.retry(exc=exc, countdown=60)

# celery beat 配置
# settings.py
CELERY_BEAT_SCHEDULE = {
    'archive-tasks-monthly': {
        'task': 'apps.tasks.tasks.archive_old_tasks',
        'schedule': 30 * 24 * 60 * 60,  # 每月执行
        'args': (3,),  # 归档3个月前的数据
    },
}
```

---

## 9. 监控告警体系

### 9.1 监控指标体系

#### 9.1.1 数据库性能指标

```python
# apps/common/monitoring.py
from django.db import connection
from django.core.cache import cache
import time

class DatabaseMonitor:
    """数据库监控器"""
    
    @staticmethod
    def get_slow_queries(limit=10, min_duration=1.0):
        """获取慢查询"""
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    DIGEST_TEXT as query,
                    COUNT_STAR as exec_count,
                    AVG_TIMER_WAIT/1000000000 as avg_latency_sec,
                    MAX_TIMER_WAIT/1000000000 as max_latency_sec,
                    SUM_ROWS_SENT as total_rows_sent,
                    SUM_ROWS_EXAMINED as total_rows_examined,
                    FIRST_SEEN,
                    LAST_SEEN
                FROM performance_schema.events_statements_summary_by_digest
                WHERE SCHEMA_NAME = DATABASE()
                AND AVG_TIMER_WAIT > %s * 1000000000
                ORDER BY AVG_TIMER_WAIT DESC
                LIMIT %s
            """, [min_duration, limit])
            
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    @staticmethod
    def get_table_stats():
        """获取表统计信息"""
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    TABLE_NAME,
                    TABLE_ROWS,
                    DATA_LENGTH,
                    INDEX_LENGTH,
                    DATA_LENGTH + INDEX_LENGTH as total_size,
                    ROUND(DATA_LENGTH / 1024 / 1024, 2) as data_size_mb,
                    ROUND(INDEX_LENGTH / 1024 / 1024, 2) as index_size_mb
                FROM information_schema.TABLES
                WHERE TABLE_SCHEMA = DATABASE()
                AND TABLE_NAME IN ('test_tasks', 'sub_tasks', 'generic_test_data')
                ORDER BY (DATA_LENGTH + INDEX_LENGTH) DESC
            """)
            
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    @staticmethod
    def get_index_usage():
        """获取索引使用情况"""
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    OBJECT_SCHEMA,
                    OBJECT_NAME,
                    INDEX_NAME,
                    COUNT_FETCH,
                    COUNT_INSERT,
                    COUNT_UPDATE,
                    COUNT_DELETE
                FROM performance_schema.table_io_waits_summary_by_index_usage
                WHERE OBJECT_SCHEMA = DATABASE()
                AND INDEX_NAME IS NOT NULL
                ORDER BY COUNT_FETCH DESC
            """)
            
            columns = [desc[0] for desc in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    @staticmethod
    def get_connection_stats():
        """获取连接统计"""
        with connection.cursor() as cursor:
            cursor.execute("SHOW STATUS LIKE 'Threads_%'")
            thread_stats = dict(cursor.fetchall())
            
            cursor.execute("SHOW STATUS LIKE 'Connections'")
            connections = cursor.fetchone()[1]
            
            cursor.execute("SHOW STATUS LIKE 'Max_used_connections'")
            max_used = cursor.fetchone()[1]
            
            return {
                'threads_connected': thread_stats.get('Threads_connected', 0),
                'threads_running': thread_stats.get('Threads_running', 0),
                'threads_cached': thread_stats.get('Threads_cached', 0),
                'total_connections': connections,
                'max_used_connections': max_used,
            }


class QueryProfiler:
    """查询性能分析器"""
    
    def __init__(self):
        self.queries = []
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, *args):
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
    
    def profile_query(self, queryset):
        """分析查询性能"""
        from django.db import connection
        
        # 清除之前的查询日志
        connection.queries_log.clear()
        
        start = time.time()
        list(queryset)  # 执行查询
        duration = time.time() - start
        
        queries = connection.queries
        return {
            'duration_ms': duration * 1000,
            'query_count': len(queries),
            'queries': queries,
        }
```

#### 9.1.2 应用性能指标

```python
# middleware/monitoring_middleware.py
import time
import logging
from django.utils.deprecation import MiddlewareMixin

logger = logging.getLogger('performance')

class PerformanceMonitoringMiddleware(MiddlewareMixin):
    """性能监控中间件"""
    
    def process_request(self, request):
        request.start_time = time.time()
        return None
    
    def process_response(self, request, response):
        if not hasattr(request, 'start_time'):
            return response
        
        duration = time.time() - request.start_time
        duration_ms = duration * 1000
        
        # 记录性能数据
        log_data = {
            'method': request.method,
            'path': request.path,
            'duration_ms': round(duration_ms, 2),
            'status_code': response.status_code,
            'user': request.user.username if request.user.is_authenticated else 'anonymous',
            'ip': self.get_client_ip(request),
        }
        
        # 慢请求告警
        if duration_ms > 1000:  # 超过1秒
            logger.warning(f"Slow request: {log_data}")
        elif duration_ms > 3000:  # 超过3秒
            logger.error(f"Very slow request: {log_data}")
        else:
            logger.info(f"Request: {log_data}")
        
        # 添加响应头
        response['X-Response-Time'] = f"{duration_ms:.2f}ms"
        
        return response
    
    def get_client_ip(self, request):
        x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
        if x_forwarded_for:
            return x_forwarded_for.split(',')[0]
        return request.META.get('REMOTE_ADDR')
```

### 9.2 告警规则配置

```python
# apps/common/alerts.py
import json
from django.core.mail import send_mail
from django.conf import settings
from django.core.cache import cache

class AlertManager:
    """告警管理器"""
    
    # 告警级别
    LEVEL_INFO = 'info'
    LEVEL_WARNING = 'warning'
    LEVEL_ERROR = 'error'
    LEVEL_CRITICAL = 'critical'
    
    # 告警渠道
    CHANNEL_EMAIL = 'email'
    CHANNEL_SMS = 'sms'
    CHANNEL_WEBHOOK = 'webhook'
    
    @classmethod
    def send_alert(cls, title, message, level=LEVEL_WARNING, data=None):
        """发送告警"""
        alert = {
            'title': title,
            'message': message,
            'level': level,
            'data': data or {},
            'timestamp': timezone.now().isoformat(),
        }
        
        # 根据级别选择渠道
        if level == cls.CRITICAL:
            cls._send_email(alert)
            cls._send_webhook(alert)
        elif level == cls.ERROR:
            cls._send_email(alert)
        elif level == cls.WARNING:
            cls._send_webhook(alert)
        
        # 记录到缓存（最近100条）
        cls._record_alert(alert)
    
    @classmethod
    def _send_email(cls, alert):
        """发送邮件告警"""
        subject = f"[{alert['level'].upper()}] {alert['title']}"
        message = json.dumps(alert, indent=2, ensure_ascii=False)
        
        send_mail(
            subject=subject,
            message=message,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=settings.ALERT_EMAIL_LIST,
            fail_silently=True,
        )
    
    @classmethod
    def _send_webhook(cls, alert):
        """发送Webhook告警"""
        import requests
        
        webhook_url = settings.ALERT_WEBHOOK_URL
        if not webhook_url:
            return
        
        try:
            requests.post(webhook_url, json=alert, timeout=5)
        except Exception as e:
            logger.error(f"Failed to send webhook alert: {e}")
    
    @classmethod
    def _record_alert(cls, alert):
        """记录告警历史"""
        key = 'alert:history'
        alerts = cache.get(key, [])
        alerts.insert(0, alert)
        alerts = alerts[:100]  # 保留最近100条
        cache.set(key, alerts, 86400)  # 24小时
    
    @classmethod
    def check_database_health(cls):
        """检查数据库健康状态"""
        from .monitoring import DatabaseMonitor
        
        # 检查慢查询
        slow_queries = DatabaseMonitor.get_slow_queries(limit=5, min_duration=2.0)
        if slow_queries:
            cls.send_alert(
                title="数据库慢查询告警",
                message=f"检测到 {len(slow_queries)} 条慢查询",
                level=cls.WARNING,
                data={'slow_queries': slow_queries}
            )
        
        # 检查连接数
        conn_stats = DatabaseMonitor.get_connection_stats()
        threads_connected = int(conn_stats.get('threads_connected', 0))
        max_connections = 200  # 配置的最大连接数
        
        if threads_connected > max_connections * 0.8:
            cls.send_alert(
                title="数据库连接数告警",
                message=f"当前连接数 {threads_connected}，超过阈值 {max_connections * 0.8}",
                level=cls.ERROR,
                data=conn_stats
            )
        
        # 检查表大小
        table_stats = DatabaseMonitor.get_table_stats()
        for stat in table_stats:
            size_mb = stat.get('total_size', 0) / 1024 / 1024
            if size_mb > 5000:  # 超过5GB
                cls.send_alert(
                    title="数据库表大小告警",
                    message=f"表 {stat['TABLE_NAME']} 大小 {size_mb:.2f}MB，超过阈值",
                    level=cls.WARNING,
                    data=stat
                )


# Celery定时监控任务
@shared_task
def health_check():
    """定时健康检查"""
    AlertManager.check_database_health()
```

### 9.3 监控Dashboard

```python
# apps/common/views.py
from django.views import View
from django.http import JsonResponse
from .monitoring import DatabaseMonitor

class MonitoringDashboardView(View):
    """监控仪表盘API"""
    
    def get(self, request):
        """获取监控数据"""
        data = {
            'database': {
                'slow_queries': DatabaseMonitor.get_slow_queries(limit=10),
                'table_stats': DatabaseMonitor.get_table_stats(),
                'index_usage': DatabaseMonitor.get_index_usage(),
                'connection_stats': DatabaseMonitor.get_connection_stats(),
            },
            'cache': {
                'hit_rate': self.get_cache_hit_rate(),
                'memory_usage': self.get_cache_memory(),
            },
            'alerts': self.get_recent_alerts(),
        }
        return JsonResponse(data)
    
    def get_cache_hit_rate(self):
        """获取缓存命中率"""
        from django_redis import get_redis_connection
        redis = get_redis_connection('default')
        info = redis.info('stats')
        hits = info.get('keyspace_hits', 0)
        misses = info.get('keyspace_misses', 0)
        total = hits + misses
        return hits / total if total > 0 else 0
    
    def get_cache_memory(self):
        """获取缓存内存使用"""
        from django_redis import get_redis_connection
        redis = get_redis_connection('default')
        info = redis.info('memory')
        return {
            'used_memory': info.get('used_memory_human'),
            'used_memory_peak': info.get('used_memory_peak_human'),
        }
    
    def get_recent_alerts(self):
        """获取最近告警"""
        from django.core.cache import cache
        return cache.get('alert:history', [])
```

---

## 10. 性能测试指标

### 10.1 性能基准测试

```python
# scripts/performance_test.py
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import requests

class PerformanceTester:
    """性能测试工具"""
    
    def __init__(self, base_url='http://localhost:8000'):
        self.base_url = base_url
        self.results = []
    
    def test_task_list_api(self, concurrent=10, requests_per_user=50):
        """测试任务列表API性能"""
        
        def worker(user_id):
            times = []
            for i in range(requests_per_user):
                start = time.time()
                response = requests.get(
                    f"{self.base_url}/api/tasks/",
                    params={'page': 1, 'page_size': 20},
                    headers={'Authorization': f'Token user_{user_id}'}
                )
                duration = (time.time() - start) * 1000
                times.append(duration)
            return times
        
        # 并发测试
        with ThreadPoolExecutor(max_workers=concurrent) as executor:
            futures = [executor.submit(worker, i) for i in range(concurrent)]
            for future in futures:
                self.results.extend(future.result())
        
        return self._analyze_results()
    
    def test_database_query(self):
        """测试数据库查询性能"""
        from django.db import connection
        from apps.tasks.models import TestTask
        
        # 测试1: 简单查询
        start = time.time()
        list(TestTask.objects.all()[:100])
        simple_time = (time.time() - start) * 1000
        
        # 测试2: 关联查询
        start = time.time()
        list(TestTask.objects.select_related(
            'test_type', 'status', 'priority'
        )[:100])
        related_time = (time.time() - start) * 1000
        
        # 测试3: 聚合查询
        start = time.time()
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT status_id, COUNT(*) 
                FROM test_tasks 
                GROUP BY status_id
            """)
        aggregate_time = (time.time() - start) * 1000
        
        return {
            'simple_query_ms': simple_time,
            'related_query_ms': related_time,
            'aggregate_query_ms': aggregate_time,
        }
    
    def _analyze_results(self):
        """分析测试结果"""
        if not self.results:
            return {}
        
        return {
            'total_requests': len(self.results),
            'avg_latency_ms': statistics.mean(self.results),
            'min_latency_ms': min(self.results),
            'max_latency_ms': max(self.results),
            'p50_latency_ms': statistics.median(self.results),
            'p95_latency_ms': self._percentile(self.results, 95),
            'p99_latency_ms': self._percentile(self.results, 99),
            'rps': len(self.results) / sum(self.results) * 1000,
        }
    
    @staticmethod
    def _percentile(data, percentile):
        """计算百分位数"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[index]


# 测试脚本
if __name__ == '__main__':
    tester = PerformanceTester()
    
    print("开始性能测试...")
    
    # API测试
    api_results = tester.test_task_list_api(concurrent=10, requests_per_user=50)
    print("\nAPI性能测试结果:")
    for key, value in api_results.items():
        print(f"  {key}: {value:.2f}")
    
    # 数据库测试
    db_results = tester.test_database_query()
    print("\n数据库查询性能:")
    for key, value in db_results.items():
        print(f"  {key}: {value:.2f}ms")
```

### 10.2 性能目标指标

| 指标 | 目标值 | 可接受值 | 当前值 | 优化策略 |
|------|--------|----------|--------|----------|
| **任务列表查询** | < 50ms | < 100ms | 2000ms | 覆盖索引 + 缓存 |
| **子任务加载** | < 30ms | < 50ms | 1500ms | prefetch_related + 索引 |
| **任务详情查询** | < 30ms | < 50ms | 800ms | select_related + 缓存 |
| **数据统计报表** | < 200ms | < 500ms | 5000ms | 物化视图 + 定时计算 |
| **并发处理能力** | 1000 RPS | 500 RPS | 100 RPS | 读写分离 + 连接池 |
| **数据库CPU** | < 50% | < 70% | 85% | 索引优化 + 查询优化 |
| **缓存命中率** | > 90% | > 80% | 60% | 优化缓存策略 |
| **数据一致性延迟** | < 1s | < 3s | 0s | 半同步复制 |

### 10.3 性能验收标准

```python
# tests/test_performance.py
import pytest
from django.test import TestCase
from django.urls import reverse
from rest_framework.test import APIClient
import time

class PerformanceRequirements(TestCase):
    """性能需求验收测试"""
    
    def setUp(self):
        self.client = APIClient()
        self.client.force_authenticate(user=self.create_test_user())
    
    def test_task_list_response_time(self):
        """任务列表响应时间 < 100ms"""
        start = time.time()
        response = self.client.get(reverse('task-list'))
        duration = (time.time() - start) * 1000
        
        self.assertEqual(response.status_code, 200)
        self.assertLess(duration, 100, 
            f"任务列表响应时间 {duration:.2f}ms 超过100ms阈值")
    
    def test_task_detail_response_time(self):
        """任务详情响应时间 < 50ms"""
        task = self.create_test_task()
        
        start = time.time()
        response = self.client.get(reverse('task-detail', args=[task.id]))
        duration = (time.time() - start) * 1000
        
        self.assertEqual(response.status_code, 200)
        self.assertLess(duration, 50,
            f"任务详情响应时间 {duration:.2f}ms 超过50ms阈值")
    
    def test_database_query_count(self):
        """单次请求数据库查询次数 < 5"""
        from django.db import connection
        
        initial_queries = len(connection.queries)
        response = self.client.get(reverse('task-list'))
        final_queries = len(connection.queries)
        
        query_count = final_queries - initial_queries
        self.assertLess(query_count, 5,
            f"请求产生了 {query_count} 次数据库查询，超过5次阈值")
    
    def test_concurrent_requests(self):
        """并发请求测试"""
        import concurrent.futures
        
        def make_request():
            start = time.time()
            response = self.client.get(reverse('task-list'))
            duration = (time.time() - start) * 1000
            return response.status_code == 200, duration
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request) for _ in range(50)]
            results = [f.result() for f in futures]
        
        success_count = sum(1 for success, _ in results if success)
        avg_duration = sum(d for _, d in results) / len(results)
        
        self.assertEqual(success_count, 50, "并发请求有失败")
        self.assertLess(avg_duration, 200,
            f"并发平均响应时间 {avg_duration:.2f}ms 超过200ms")
```

---

## 11. 实施路线图

### 11.1 第一阶段：索引优化与查询优化（1-2周）

**目标**: 快速提升查询性能，解决80%的慢查询问题

**任务清单**:
- [ ] 创建缺失的外键索引
- [ ] 创建业务查询索引
- [ ] 创建覆盖索引
- [ ] 优化Django ORM查询（select_related/prefetch_related）
- [ ] 添加查询计数监控
- [ ] 性能基准测试

**预期收益**:
- 查询性能提升 5-10 倍
- 数据库CPU降低 30-40%

### 11.2 第二阶段：缓存与读写分离（2-4周）

**目标**: 减少数据库负载，提升并发能力

**任务清单**:
- [ ] 部署Redis集群
- [ ] 实现多级缓存架构
- [ ] 配置数据库主从复制
- [ ] 实现读写分离路由
- [ ] 缓存策略实施（热点数据缓存）
- [ ] 缓存一致性保障

**预期收益**:
- 数据库读负载降低 60-70%
- 并发能力提升 3-5 倍

### 11.3 第三阶段：数据分区与归档（4-8周）

**目标**: 管理数据增长，保持查询性能

**任务清单**:
- [ ] 实施表分区策略
- [ ] 创建归档表
- [ ] 开发归档脚本
- [ ] 实施自动归档任务
- [ ] 数据归档验证
- [ ] 历史数据查询优化

**预期收益**:
- 单表数据量减少 50-70%
- 查询性能稳定在毫秒级

### 11.4 第四阶段：监控与持续优化（持续）

**目标**: 建立完善的监控体系，持续优化

**任务清单**:
- [ ] 部署监控仪表盘
- [ ] 配置告警规则
- [ ] 建立慢查询分析流程
- [ ] 定期索引优化
- [ ] 容量规划
- [ ] 性能回归测试

### 11.5 实施风险与应对

| 风险 | 可能性 | 影响 | 应对措施 |
|------|--------|------|----------|
| 索引创建锁表 | 中 | 高 | 使用pt-online-schema-change，选择低峰期执行 |
| 缓存雪崩 | 低 | 高 | 多级缓存 + 熔断机制 + 随机TTL |
| 主从延迟 | 中 | 中 | 半同步复制 + 强制主库读取策略 |
| 数据丢失 | 低 | 极高 | 完善备份策略 + 归档验证 |
| 性能回退 | 中 | 高 | 灰度发布 + 快速回滚机制 |

---

## 12. 附录

### 12.1 关键SQL脚本汇总

```sql
-- 查看表大小
SELECT 
    TABLE_NAME,
    ROUND(DATA_LENGTH / 1024 / 1024, 2) AS data_mb,
    ROUND(INDEX_LENGTH / 1024 / 1024, 2) AS index_mb,
    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS total_mb,
    TABLE_ROWS
FROM information_schema.TABLES
WHERE TABLE_SCHEMA = 'laboratory_test_management'
ORDER BY total_mb DESC;

-- 查看索引使用情况
SELECT 
    TABLE_NAME,
    INDEX_NAME,
    CARDINALITY,
    SUB_PART,
    INDEX_TYPE
FROM information_schema.STATISTICS
WHERE TABLE_SCHEMA = 'laboratory_test_management'
AND TABLE_NAME IN ('test_tasks', 'sub_tasks')
ORDER BY TABLE_NAME, INDEX_NAME;

-- 查看慢查询
SELECT 
    DIGEST_TEXT,
    COUNT_STAR,
    AVG_TIMER_WAIT / 1000000000 AS avg_latency_sec,
    MAX_TIMER_WAIT / 1000000000 AS max_latency_sec,
    SUM_ROWS_SENT,
    SUM_ROWS_EXAMINED
FROM performance_schema.events_statements_summary_by_digest
WHERE SCHEMA_NAME = 'laboratory_test_management'
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 20;

-- 查看锁等待
SELECT 
    r.trx_id waiting_trx_id,
    r.trx_mysql_thread_id waiting_thread,
    r.trx_query waiting_query,
    b.trx_id blocking_trx_id,
    b.trx_mysql_thread_id blocking_thread
FROM information_schema.innodb_lock_waits w
INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;
```

### 12.2 Django模型优化示例

```python
# apps/tasks/models_optimized.py

class TestTaskOptimized(models.Model):
    """优化后的试验任务模型"""
    
    # ... 字段定义 ...
    
    class Meta:
        db_table = 'test_tasks'
        verbose_name = '试验任务'
        verbose_name_plural = '试验任务'
        ordering = ['-created_at']
        
        # 数据库级索引
        indexes = [
            # 基本查询索引
            models.Index(fields=['status', '-created_at'], name='idx_status_created'),
            models.Index(fields=['assignee', 'status', '-created_at'], name='idx_assignee_status'),
            models.Index(fields=['test_type', '-created_at'], name='idx_type_created'),
            
            # 时间索引
            models.Index(fields=['start_date'], name='idx_start_date'),
            models.Index(fields=['end_date', 'status'], name='idx_end_status'),
            
            # 覆盖索引（列表查询）
            models.Index(
                fields=['status', '-created_at', 'id', 'assignee', 'priority'],
                name='idx_list_cover'
            ),
            
            # 分区键索引
            models.Index(fields=['partition_key', '-created_at'], name='idx_partition'),
        ]
        
        # 复合唯一约束
        unique_together = [
            ['task_number'],
        ]
```

### 12.3 性能测试报告模板

```markdown
# 性能测试报告

**测试日期**: YYYY-MM-DD  
**测试环境**: [环境描述]  
**数据规模**: [数据量]

## 测试结果

| 测试项 | 目标 | 实际 | 状态 |
|--------|------|------|------|
| 任务列表查询 | < 100ms | XXms | ✓/✗ |
| 子任务加载 | < 50ms | XXms | ✓/✗ |
| 并发能力 | > 500 RPS | XX RPS | ✓/✗ |
| ... | ... | ... | ... |

## 结论

[结论描述]

## 优化建议

[建议列表]
```

---

**文档结束**

---

*本文档由技术团队维护，如有疑问请联系数据库管理员或架构师。*

# è¯•éªŒå®¤ç®¡ç†ç³»ç»Ÿæ•°æ®åº“ä¼˜åŒ–æ–¹æ¡ˆï¼ˆç™¾ä¸‡çº§æ•°æ®ï¼‰

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **æ›´æ–°æ—¥æœŸ**: 2026-02-05  
> **é€‚ç”¨å¯¹è±¡**: TestTaskï¼ˆä¸»ä»»åŠ¡ï¼‰ã€SubTaskï¼ˆå­ä»»åŠ¡ï¼‰è¡¨  
> **ç›®æ ‡æ•°æ®é‡**: 100ä¸‡+ è®°å½•

---

## ğŸ“Š ä¸€ã€å½“å‰æ•°æ®åº“ç»“æ„åˆ†æ

### 1.1 TestTaskï¼ˆä¸»ä»»åŠ¡è¡¨ï¼‰ç»“æ„

```sql
CREATE TABLE `test_tasks` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `task_number` varchar(50) NOT NULL,
  `task_name` varchar(200) NOT NULL,
  `product_name` varchar(50) DEFAULT '',
  `product_model` varchar(20) DEFAULT '',
  `test_type_id` bigint(20) NOT NULL,
  `priority_id` bigint(20) NOT NULL,
  `status_id` bigint(20) NOT NULL,
  `requester_id` bigint(20) NOT NULL,
  `assignee_id` bigint(20) DEFAULT NULL,
  `requester_name` varchar(100) DEFAULT '',
  `requester_phone` varchar(20) DEFAULT '',
  `requester_department` varchar(100) DEFAULT '',
  `description` longtext,
  `test_outline` longtext,
  `test_report` longtext,
  `test_outline_file` varchar(100) DEFAULT NULL,
  `test_report_file` varchar(100) DEFAULT NULL,
  `start_date` date NOT NULL,
  `end_date` date NOT NULL,
  `actual_start_date` date DEFAULT NULL,
  `actual_end_date` date DEFAULT NULL,
  `rejection_reason` longtext DEFAULT '',
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `task_number` (`task_number`),
  KEY `test_tasks_test_type_id_xxx` (`test_type_id`),
  KEY `test_tasks_priority_id_xxx` (`priority_id`),
  KEY `test_tasks_status_id_xxx` (`status_id`),
  KEY `test_tasks_requester_id_xxx` (`requester_id`),
  KEY `test_tasks_assignee_id_xxx` (`assignee_id`),
  CONSTRAINT `fk_test_tasks_test_type` FOREIGN KEY (`test_type_id`) REFERENCES `test_types` (`id`),
  CONSTRAINT `fk_test_tasks_priority` FOREIGN KEY (`priority_id`) REFERENCES `priority_types` (`id`),
  CONSTRAINT `fk_test_tasks_status` FOREIGN KEY (`status_id`) REFERENCES `task_status` (`id`),
  CONSTRAINT `fk_test_tasks_requester` FOREIGN KEY (`requester_id`) REFERENCES `users` (`id`),
  CONSTRAINT `fk_test_tasks_assignee` FOREIGN KEY (`assignee_id`) REFERENCES `users` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

**å½“å‰é—®é¢˜ï¼š**
- ç¼ºå°‘å¤åˆç´¢å¼•
- æ—¥æœŸå­—æ®µæœªå»ºç«‹ç´¢å¼•
- æ–‡æœ¬æœç´¢æ•ˆç‡ä½
- æ— åˆ†åŒºç­–ç•¥

### 1.2 SubTaskï¼ˆå­ä»»åŠ¡è¡¨ï¼‰ç»“æ„

```sql
CREATE TABLE `sub_tasks` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `parent_task_id` bigint(20) NOT NULL,
  `subtask_number` varchar(50) NOT NULL,
  `subtask_name` varchar(200) NOT NULL,
  `test_type_id` bigint(20) NOT NULL,
  `status_id` bigint(20) NOT NULL,
  `assignee_id` bigint(20) DEFAULT NULL,
  `description` longtext,
  `start_date` date NOT NULL,
  `end_date` date NOT NULL,
  `actual_start_date` date DEFAULT NULL,
  `actual_end_date` date DEFAULT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `subtask_number` (`subtask_number`),
  KEY `sub_tasks_parent_task_id_xxx` (`parent_task_id`),
  KEY `sub_tasks_test_type_id_xxx` (`test_type_id`),
  KEY `sub_tasks_status_id_xxx` (`status_id`),
  KEY `sub_tasks_assignee_id_xxx` (`assignee_id`),
  CONSTRAINT `fk_sub_tasks_parent` FOREIGN KEY (`parent_task_id`) REFERENCES `test_tasks` (`id`),
  CONSTRAINT `fk_sub_tasks_test_type` FOREIGN KEY (`test_type_id`) REFERENCES `test_types` (`id`),
  CONSTRAINT `fk_sub_tasks_status` FOREIGN KEY (`status_id`) REFERENCES `task_status` (`id`),
  CONSTRAINT `fk_sub_tasks_assignee` FOREIGN KEY (`assignee_id`) REFERENCES `users` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

**å½“å‰é—®é¢˜ï¼š**
- å¤åˆç´¢å¼•ä¸è¶³
- æœªæŒ‰parent_taskåˆ†åŒº
- ç¼ºå°‘è¦†ç›–ç´¢å¼•

---

## ğŸ¯ äºŒã€ç™¾ä¸‡çº§æ•°æ®ä¼˜åŒ–ç­–ç•¥

### 2.1 ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

#### 2.1.1 TestTaskè¡¨ç´¢å¼•ä¼˜åŒ–

**æ–°å¢å¤åˆç´¢å¼•ï¼š**

```sql
-- 1. ä»»åŠ¡åˆ—è¡¨æŸ¥è¯¢ä¼˜åŒ–ï¼ˆæŒ‰çŠ¶æ€+åˆ›å»ºæ—¶é—´å€’åºï¼Œæœ€å¸¸ç”¨çš„æŸ¥è¯¢ï¼‰
CREATE INDEX `idx_test_tasks_status_created` 
ON `test_tasks` (`status_id`, `created_at` DESC);

-- 2. ç”¨æˆ·ä»»åŠ¡æŸ¥è¯¢ä¼˜åŒ–ï¼ˆç”³è¯·äºº+çŠ¶æ€ï¼‰
CREATE INDEX `idx_test_tasks_requester_status` 
ON `test_tasks` (`requester_id`, `status_id`, `created_at` DESC);

-- 3. è´Ÿè´£äººä»»åŠ¡æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX `idx_test_tasks_assignee_status` 
ON `test_tasks` (`assignee_id`, `status_id`, `created_at` DESC);

-- 4. æ—¥æœŸèŒƒå›´æŸ¥è¯¢ä¼˜åŒ–ï¼ˆé€¾æœŸä»»åŠ¡æŸ¥è¯¢ï¼‰
CREATE INDEX `idx_test_tasks_end_date_status` 
ON `test_tasks` (`end_date`, `status_id`);

-- 5. è¯•éªŒç±»å‹+çŠ¶æ€æŸ¥è¯¢
CREATE INDEX `idx_test_tasks_type_status` 
ON `test_tasks` (`test_type_id`, `status_id`, `created_at` DESC);

-- 6. ä¼˜å…ˆçº§+çŠ¶æ€æŸ¥è¯¢
CREATE INDEX `idx_test_tasks_priority_status` 
ON `test_tasks` (`priority_id`, `status_id`, `created_at` DESC);

-- 7. ä»»åŠ¡ç¼–å·å‰ç¼€æŸ¥è¯¢ï¼ˆç”¨äºæœç´¢ï¼‰
CREATE INDEX `idx_test_tasks_number_prefix` 
ON `test_tasks` (`task_number`(10));

-- 8. äº§å“åç§°æœç´¢ä¼˜åŒ–ï¼ˆå…¨æ–‡ç´¢å¼•ï¼ŒMySQL 5.6+ï¼‰
CREATE FULLTEXT INDEX `idx_test_tasks_product_name` 
ON `test_tasks` (`product_name`);

-- 9. è¦†ç›–ç´¢å¼•ï¼ˆå‡å°‘å›è¡¨æŸ¥è¯¢ï¼‰
CREATE INDEX `idx_test_tasks_cover_status` 
ON `test_tasks` (`status_id`, `task_number`, `task_name`, `created_at`);
```

#### 2.1.2 SubTaskè¡¨ç´¢å¼•ä¼˜åŒ–

```sql
-- 1. ä¸»ä»»åŠ¡ä¸‹çš„å­ä»»åŠ¡æŸ¥è¯¢ï¼ˆæœ€å¸¸ç”¨çš„æŸ¥è¯¢ï¼‰
CREATE INDEX `idx_sub_tasks_parent_status` 
ON `sub_tasks` (`parent_task_id`, `status_id`, `created_at` DESC);

-- 2. è´Ÿè´£äººå­ä»»åŠ¡æŸ¥è¯¢
CREATE INDEX `idx_sub_tasks_assignee_status` 
ON `sub_tasks` (`assignee_id`, `status_id`, `created_at` DESC);

-- 3. è¯•éªŒç±»å‹+çŠ¶æ€æŸ¥è¯¢
CREATE INDEX `idx_sub_tasks_type_status` 
ON `sub_tasks` (`test_type_id`, `status_id`, `created_at` DESC);

-- 4. æ—¥æœŸèŒƒå›´æŸ¥è¯¢
CREATE INDEX `idx_sub_tasks_dates` 
ON `sub_tasks` (`start_date`, `end_date`);

-- 5. å­ä»»åŠ¡ç¼–å·å‰ç¼€æŸ¥è¯¢
CREATE INDEX `idx_sub_tasks_number_prefix` 
ON `sub_tasks` (`subtask_number`(15));

-- 6. è¦†ç›–ç´¢å¼•
CREATE INDEX `idx_sub_tasks_cover_parent` 
ON `sub_tasks` (`parent_task_id`, `subtask_number`, `subtask_name`, `status_id`);
```

#### 2.1.3 Djangoæ¨¡å‹è¿ç§»æ–‡ä»¶

**åˆ›å»ºæ–°çš„è¿ç§»æ–‡ä»¶ï¼š** `apps/tasks/migrations/0014_add_performance_indexes.py`

```python
from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ('tasks', '0013_previous_migration'),
    ]

    operations = [
        # TestTaskè¡¨ç´¢å¼•
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['status', '-created_at'],
                name='idx_test_task_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['requester', 'status', '-created_at'],
                name='idx_test_task_req_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['assignee', 'status', '-created_at'],
                name='idx_test_task_assign_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['end_date', 'status'],
                name='idx_test_task_enddate_status'
            ),
        ),
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['test_type', 'status', '-created_at'],
                name='idx_test_task_type_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='testtask',
            index=models.Index(
                fields=['priority', 'status', '-created_at'],
                name='idx_test_task_priority_status_created'
            ),
        ),
        
        # SubTaskè¡¨ç´¢å¼•
        migrations.AddIndex(
            model_name='subtask',
            index=models.Index(
                fields=['parent_task', 'status', '-created_at'],
                name='idx_sub_task_parent_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='subtask',
            index=models.Index(
                fields=['assignee', 'status', '-created_at'],
                name='idx_sub_task_assign_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='subtask',
            index=models.Index(
                fields=['test_type', 'status', '-created_at'],
                name='idx_sub_task_type_status_created'
            ),
        ),
        migrations.AddIndex(
            model_name='subtask',
            index=models.Index(
                fields=['start_date', 'end_date'],
                name='idx_sub_task_dates'
            ),
        ),
    ]
```

### 2.2 è¡¨åˆ†åŒºç­–ç•¥

#### 2.2.1 æŒ‰æ—¶é—´èŒƒå›´åˆ†åŒºï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** æ•°æ®æŒ‰æ—¶é—´è‡ªç„¶å¢é•¿ï¼Œå†å²æ•°æ®æŸ¥è¯¢è¾ƒå°‘

```sql
-- TestTaskè¡¨æŒ‰created_atæŒ‰æœˆåˆ†åŒº
CREATE TABLE `test_tasks_partitioned` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `task_number` varchar(50) NOT NULL,
  `task_name` varchar(200) NOT NULL,
  `product_name` varchar(50) DEFAULT '',
  `product_model` varchar(20) DEFAULT '',
  `test_type_id` bigint(20) NOT NULL,
  `priority_id` bigint(20) NOT NULL,
  `status_id` bigint(20) NOT NULL,
  `requester_id` bigint(20) NOT NULL,
  `assignee_id` bigint(20) DEFAULT NULL,
  `requester_name` varchar(100) DEFAULT '',
  `requester_phone` varchar(20) DEFAULT '',
  `requester_department` varchar(100) DEFAULT '',
  `description` longtext,
  `test_outline` longtext,
  `test_report` longtext,
  `test_outline_file` varchar(100) DEFAULT NULL,
  `test_report_file` varchar(100) DEFAULT NULL,
  `start_date` date NOT NULL,
  `end_date` date NOT NULL,
  `actual_start_date` date DEFAULT NULL,
  `actual_end_date` date DEFAULT NULL,
  `rejection_reason` longtext DEFAULT '',
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`, `created_at`),  -- åˆ†åŒºé”®å¿…é¡»æ˜¯ä¸»é”®çš„ä¸€éƒ¨åˆ†
  UNIQUE KEY `task_number` (`task_number`),
  KEY `idx_status_created` (`status_id`, `created_at`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
PARTITION BY RANGE (YEAR(created_at) * 100 + MONTH(created_at)) (
  PARTITION p202401 VALUES LESS THAN (202402),
  PARTITION p202402 VALUES LESS THAN (202403),
  PARTITION p202403 VALUES LESS THAN (202404),
  PARTITION p202404 VALUES LESS THAN (202405),
  PARTITION p202405 VALUES LESS THAN (202406),
  PARTITION p202406 VALUES LESS THAN (202407),
  PARTITION p202407 VALUES LESS THAN (202408),
  PARTITION p202408 VALUES LESS THAN (202409),
  PARTITION p202409 VALUES LESS THAN (202410),
  PARTITION p202410 VALUES LESS THAN (202411),
  PARTITION p202411 VALUES LESS THAN (202412),
  PARTITION p202412 VALUES LESS THAN (202501),
  PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- SubTaskè¡¨åŒæ ·æŒ‰æ—¶é—´åˆ†åŒº
CREATE TABLE `sub_tasks_partitioned` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `parent_task_id` bigint(20) NOT NULL,
  `subtask_number` varchar(50) NOT NULL,
  `subtask_name` varchar(200) NOT NULL,
  `test_type_id` bigint(20) NOT NULL,
  `status_id` bigint(20) NOT NULL,
  `assignee_id` bigint(20) DEFAULT NULL,
  `description` longtext,
  `start_date` date NOT NULL,
  `end_date` date NOT NULL,
  `actual_start_date` date DEFAULT NULL,
  `actual_end_date` date DEFAULT NULL,
  `created_at` datetime(6) NOT NULL,
  `updated_at` datetime(6) NOT NULL,
  PRIMARY KEY (`id`, `created_at`),
  UNIQUE KEY `subtask_number` (`subtask_number`),
  KEY `idx_parent_status` (`parent_task_id`, `status_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
PARTITION BY RANGE (YEAR(created_at) * 100 + MONTH(created_at)) (
  PARTITION p202401 VALUES LESS THAN (202402),
  PARTITION p202402 VALUES LESS THAN (202403),
  -- ... æ›´å¤šåˆ†åŒº
  PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

**åˆ†åŒºç»´æŠ¤è„šæœ¬ï¼š**

```sql
-- æ·»åŠ æ–°åˆ†åŒºï¼ˆæ¯æœˆæ‰§è¡Œä¸€æ¬¡ï¼‰
ALTER TABLE test_tasks_partitioned 
ADD PARTITION (
  PARTITION p202501 VALUES LESS THAN (202502)
);

-- æŸ¥çœ‹åˆ†åŒºæƒ…å†µ
SELECT 
  partition_name,
  from_days(partition_description) as partition_range,
  table_rows
FROM information_schema.partitions
WHERE table_name = 'test_tasks_partitioned';

-- æŸ¥è¯¢ç‰¹å®šåˆ†åŒºï¼ˆæå‡æŸ¥è¯¢æ€§èƒ½ï¼‰
SELECT * FROM test_tasks_partitioned 
WHERE created_at >= '2024-01-01' AND created_at < '2024-02-01';
```

#### 2.2.2 Djangoåˆ†åŒºå®ç°æ–¹æ¡ˆ

ç”±äºDjango ORMåŸç”Ÿä¸æ”¯æŒåˆ†åŒºï¼Œéœ€è¦è‡ªå®šä¹‰ç®¡ç†å™¨ï¼š

```python
# apps/tasks/managers.py
from django.db import models
from django.db.models import Q


class PartitionedTaskManager(models.Manager):
    """æ”¯æŒåˆ†åŒºçš„ä»»åŠ¡ç®¡ç†å™¨"""
    
    def get_partition_table(self, year, month):
        """è·å–æŒ‡å®šæœˆä»½çš„åˆ†åŒºè¡¨å"""
        return f"test_tasks_{year}{month:02d}"
    
    def get_recent_partitions(self, months=3):
        """è·å–æœ€è¿‘Nä¸ªæœˆçš„åˆ†åŒº"""
        from datetime import datetime, timedelta
        partitions = []
        now = datetime.now()
        for i in range(months):
            date = now - timedelta(days=30*i)
            partitions.append(self.get_partition_table(date.year, date.month))
        return partitions
    
    def filter_by_date_range(self, start_date, end_date):
        """æŒ‰æ—¥æœŸèŒƒå›´æŸ¥è¯¢ï¼ˆè‡ªåŠ¨é€‰æ‹©åˆ†åŒºï¼‰"""
        return self.filter(
            created_at__gte=start_date,
            created_at__lte=end_date
        )


class PartitionedSubTaskManager(models.Manager):
    """æ”¯æŒåˆ†åŒºçš„å­ä»»åŠ¡ç®¡ç†å™¨"""
    
    def get_by_parent_task(self, parent_task_id, status=None):
        """è·å–ä¸»ä»»åŠ¡ä¸‹çš„å­ä»»åŠ¡ï¼ˆä¼˜åŒ–æŸ¥è¯¢ï¼‰"""
        queryset = self.filter(parent_task_id=parent_task_id)
        if status:
            queryset = queryset.filter(status_id=status)
        return queryset.select_related('test_type', 'status', 'assignee').order_by('subtask_number')
```

### 2.3 æ•°æ®å½’æ¡£ç­–ç•¥

#### 2.3.1 å½’æ¡£æ–¹æ¡ˆè®¾è®¡

**å½’æ¡£è§„åˆ™ï¼š**
- å·²å®Œæˆçš„ä»»åŠ¡ï¼ˆcompleted/reviewedï¼‰è¶…è¿‡2å¹´å½’æ¡£
- å·²å–æ¶ˆçš„ä»»åŠ¡è¶…è¿‡1å¹´å½’æ¡£
- å½’æ¡£è¡¨ç»“æ„ä¸ä¸»è¡¨ç›¸åŒ

```sql
-- åˆ›å»ºå½’æ¡£è¡¨
CREATE TABLE `test_tasks_archive` LIKE `test_tasks`;
CREATE TABLE `sub_tasks_archive` LIKE `sub_tasks`;

-- æ·»åŠ å½’æ¡£æ—¶é—´å­—æ®µ
ALTER TABLE `test_tasks_archive` 
ADD COLUMN `archived_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
ADD INDEX `idx_archived_at` (`archived_at`);

ALTER TABLE `sub_tasks_archive` 
ADD COLUMN `archived_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
ADD INDEX `idx_archived_at` (`archived_at`);
```

#### 2.3.2 å½’æ¡£è„šæœ¬

```python
# scripts/archive_old_tasks.py
#!/usr/bin/env python3
"""
æ•°æ®å½’æ¡£è„šæœ¬
æ¯æœˆæ‰§è¡Œä¸€æ¬¡ï¼Œå½’æ¡£å†å²æ•°æ®
"""

import os
import sys
import django
from datetime import datetime, timedelta

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'laboratory_management.settings')
sys.path.insert(0, '/opt/laboratory_management')
django.setup()

from django.db import connection
from apps.tasks.models import TestTask, SubTask


def archive_completed_tasks():
    """å½’æ¡£å·²å®Œæˆ/å·²å®¡æ ¸çš„ä»»åŠ¡"""
    cutoff_date = datetime.now() - timedelta(days=730)  # 2å¹´å‰
    
    with connection.cursor() as cursor:
        # å½’æ¡£ä¸»ä»»åŠ¡
        cursor.execute("""
            INSERT INTO test_tasks_archive 
            SELECT *, NOW() as archived_at
            FROM test_tasks t
            INNER JOIN task_status s ON t.status_id = s.id
            WHERE s.code IN ('completed', 'reviewed')
            AND t.created_at < %s
            LIMIT 10000;
        """, [cutoff_date])
        
        archived_count = cursor.rowcount
        
        # åˆ é™¤å·²å½’æ¡£çš„ä¸»ä»»åŠ¡
        cursor.execute("""
            DELETE t FROM test_tasks t
            INNER JOIN task_status s ON t.status_id = s.id
            WHERE s.code IN ('completed', 'reviewed')
            AND t.created_at < %s
            LIMIT 10000;
        """, [cutoff_date])
        
        print(f"å·²å½’æ¡£ {archived_count} ä¸ªå®Œæˆçš„ä»»åŠ¡")
        
        # å½’æ¡£å­ä»»åŠ¡
        cursor.execute("""
            INSERT INTO sub_tasks_archive 
            SELECT *, NOW() as archived_at
            FROM sub_tasks st
            INNER JOIN task_status s ON st.status_id = s.id
            WHERE s.code IN ('completed', 'reviewed')
            AND st.created_at < %s
            LIMIT 10000;
        """, [cutoff_date])
        
        sub_archived_count = cursor.rowcount
        
        # åˆ é™¤å·²å½’æ¡£çš„å­ä»»åŠ¡
        cursor.execute("""
            DELETE st FROM sub_tasks st
            INNER JOIN task_status s ON st.status_id = s.id
            WHERE s.code IN ('completed', 'reviewed')
            AND st.created_at < %s
            LIMIT 10000;
        """, [cutoff_date])
        
        print(f"å·²å½’æ¡£ {sub_archived_count} ä¸ªå®Œæˆçš„å­ä»»åŠ¡")


def archive_cancelled_tasks():
    """å½’æ¡£å·²å–æ¶ˆçš„ä»»åŠ¡"""
    cutoff_date = datetime.now() - timedelta(days=365)  # 1å¹´å‰
    
    with connection.cursor() as cursor:
        # å½’æ¡£å·²å–æ¶ˆçš„ä¸»ä»»åŠ¡
        cursor.execute("""
            INSERT INTO test_tasks_archive 
            SELECT *, NOW() as archived_at
            FROM test_tasks t
            INNER JOIN task_status s ON t.status_id = s.id
            WHERE s.code = 'cancelled'
            AND t.created_at < %s
            LIMIT 5000;
        """, [cutoff_date])
        
        archived_count = cursor.rowcount
        
        cursor.execute("""
            DELETE t FROM test_tasks t
            INNER JOIN task_status s ON t.status_id = s.id
            WHERE s.code = 'cancelled'
            AND t.created_at < %s
            LIMIT 5000;
        """, [cutoff_date])
        
        print(f"å·²å½’æ¡£ {archived_count} ä¸ªå–æ¶ˆçš„ä»»åŠ¡")


def optimize_tables():
    """ä¼˜åŒ–è¡¨æ€§èƒ½"""
    with connection.cursor() as cursor:
        cursor.execute("OPTIMIZE TABLE test_tasks;")
        cursor.execute("OPTIMIZE TABLE sub_tasks;")
        cursor.execute("ANALYZE TABLE test_tasks;")
        cursor.execute("ANALYZE TABLE sub_tasks;")
        print("è¡¨ä¼˜åŒ–å®Œæˆ")


if __name__ == '__main__':
    print(f"å¼€å§‹æ•°æ®å½’æ¡£ - {datetime.now()}")
    
    try:
        archive_completed_tasks()
        archive_cancelled_tasks()
        optimize_tables()
        print(f"å½’æ¡£å®Œæˆ - {datetime.now()}")
    except Exception as e:
        print(f"å½’æ¡£å¤±è´¥: {e}")
        sys.exit(1)
```

**å®šæ—¶ä»»åŠ¡é…ç½®ï¼ˆcrontabï¼‰ï¼š**

```bash
# æ¯æœˆ1æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œå½’æ¡£
0 2 1 * * cd /opt/laboratory_management && /opt/laboratory_management/venv/bin/python scripts/archive_old_tasks.py >> /var/log/task_archive.log 2>&1
```

### 2.4 æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥

#### 2.4.1 DjangoæŸ¥è¯¢ä¼˜åŒ–

```python
# apps/tasks/optimized_queries.py
"""
ç™¾ä¸‡çº§æ•°æ®æŸ¥è¯¢ä¼˜åŒ–ç¤ºä¾‹
"""

from django.db import models
from django.db.models import Prefetch, Q
from django.core.cache import cache
from .models import TestTask, SubTask


class OptimizedTaskQueries:
    """ä¼˜åŒ–çš„ä»»åŠ¡æŸ¥è¯¢ç±»"""
    
    @staticmethod
    def get_task_list_by_status(status_id, page=1, page_size=20):
        """
        æŒ‰çŠ¶æ€è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        ä½¿ç”¨è¦†ç›–ç´¢å¼•å‡å°‘å›è¡¨æŸ¥è¯¢
        """
        cache_key = f'task_list_status_{status_id}_page_{page}'
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        # åªæŸ¥è¯¢å¿…è¦å­—æ®µï¼Œä½¿ç”¨select_relatedé¿å…N+1æŸ¥è¯¢
        queryset = TestTask.objects.select_related(
            'test_type',
            'priority', 
            'status',
            'requester',
            'assignee'
        ).only(
            'id',
            'task_number',
            'task_name',
            'product_name',
            'created_at',
            'start_date',
            'end_date',
            'test_type__name',
            'priority__name',
            'status__name',
            'requester__username',
            'assignee__username'
        ).filter(
            status_id=status_id
        ).order_by('-created_at')
        
        # ä½¿ç”¨åˆ†é¡µ
        start = (page - 1) * page_size
        end = start + page_size
        result = list(queryset[start:end])
        
        # ç¼“å­˜5åˆ†é’Ÿ
        cache.set(cache_key, result, timeout=300)
        
        return result
    
    @staticmethod
    def get_user_tasks(user_id, role='requester', status_id=None, limit=100):
        """
        è·å–ç”¨æˆ·çš„ä»»åŠ¡åˆ—è¡¨
        role: 'requester' æˆ– 'assignee'
        """
        cache_key = f'user_tasks_{user_id}_{role}_{status_id}'
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        filter_kwargs = {}
        if role == 'requester':
            filter_kwargs['requester_id'] = user_id
        else:
            filter_kwargs['assignee_id'] = user_id
        
        if status_id:
            filter_kwargs['status_id'] = status_id
        
        queryset = TestTask.objects.select_related(
            'test_type', 'priority', 'status'
        ).filter(
            **filter_kwargs
        ).order_by('-created_at')[:limit]
        
        result = list(queryset)
        cache.set(cache_key, result, timeout=300)
        
        return result
    
    @staticmethod
    def get_subtasks_by_parent(parent_task_id):
        """
        è·å–ä¸»ä»»åŠ¡ä¸‹çš„å­ä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        """
        cache_key = f'subtasks_parent_{parent_task_id}'
        cached_result = cache.get(cache_key)
        
        if cached_result:
            return cached_result
        
        queryset = SubTask.objects.select_related(
            'test_type',
            'status',
            'assignee'
        ).filter(
            parent_task_id=parent_task_id
        ).order_by('subtask_number')
        
        result = list(queryset)
        cache.set(cache_key, result, timeout=600)  # ç¼“å­˜10åˆ†é’Ÿ
        
        return result
    
    @staticmethod
    def search_tasks_by_product_name(keyword, limit=50):
        """
        æŒ‰äº§å“åç§°æœç´¢ï¼ˆä½¿ç”¨å…¨æ–‡ç´¢å¼•ï¼‰
        """
        # MySQLå…¨æ–‡æœç´¢
        from django.db.models import SearchQuery, SearchRank, SearchVector
        
        queryset = TestTask.objects.annotate(
            rank=SearchRank(
                SearchVector('product_name'),
                SearchQuery(keyword)
            )
        ).filter(
            rank__gte=0.1
        ).order_by('-rank')[:limit]
        
        return list(queryset)
    
    @staticmethod
    def get_overdue_tasks():
        """
        è·å–é€¾æœŸä»»åŠ¡ï¼ˆä½¿ç”¨ç´¢å¼•ä¼˜åŒ–ï¼‰
        """
        from django.utils import timezone
        today = timezone.now().date()
        
        # ä½¿ç”¨æ—¥æœŸç´¢å¼•
        queryset = TestTask.objects.select_related(
            'status', 'assignee'
        ).filter(
            end_date__lt=today,
            status__code__in=['pending', 'in_progress', 'pending_review']
        ).order_by('end_date')
        
        return list(queryset)


class BulkOperations:
    """æ‰¹é‡æ“ä½œä¼˜åŒ–"""
    
    @staticmethod
    def bulk_create_subtasks(subtask_data_list, batch_size=1000):
        """
        æ‰¹é‡åˆ›å»ºå­ä»»åŠ¡
        """
        from django.db import transaction
        
        with transaction.atomic():
            SubTask.objects.bulk_create(
                subtask_data_list,
                batch_size=batch_size,
                ignore_conflicts=True
            )
    
    @staticmethod
    def bulk_update_task_status(task_ids, new_status_id):
        """
        æ‰¹é‡æ›´æ–°ä»»åŠ¡çŠ¶æ€
        """
        from django.db import transaction
        
        with transaction.atomic():
            TestTask.objects.filter(
                id__in=task_ids
            ).update(status_id=new_status_id)
```

#### 2.4.2 åŸå§‹SQLä¼˜åŒ–æŸ¥è¯¢

```python
# å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹SQL
from django.db import connection

def get_task_statistics():
    """
    è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ä¼˜åŒ–SQLï¼‰
    """
    with connection.cursor() as cursor:
        cursor.execute("""
            SELECT 
                s.name as status_name,
                s.code as status_code,
                COUNT(*) as task_count,
                MIN(t.created_at) as oldest_task,
                MAX(t.created_at) as newest_task
            FROM test_tasks t
            INNER JOIN task_status s ON t.status_id = s.id
            WHERE t.created_at >= DATE_SUB(NOW(), INTERVAL 1 YEAR)
            GROUP BY s.id, s.name, s.code
            ORDER BY task_count DESC;
        """)
        
        columns = [col[0] for col in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]


def get_user_workload():
    """
    è·å–ç”¨æˆ·å·¥ä½œé‡ç»Ÿè®¡
    """
    with connection.cursor() as cursor:
        cursor.execute("""
            SELECT 
                u.username,
                u.employee_id,
                COUNT(DISTINCT t.id) as assigned_tasks,
                COUNT(DISTINCT st.id) as assigned_subtasks,
                SUM(CASE WHEN t.end_date < CURDATE() AND s.code NOT IN ('completed', 'reviewed', 'cancelled') THEN 1 ELSE 0 END) as overdue_tasks
            FROM users u
            LEFT JOIN test_tasks t ON u.id = t.assignee_id AND t.created_at >= DATE_SUB(NOW(), INTERVAL 3 MONTH)
            LEFT JOIN task_status s ON t.status_id = s.id
            LEFT JOIN sub_tasks st ON u.id = st.assignee_id AND st.created_at >= DATE_SUB(NOW(), INTERVAL 3 MONTH)
            WHERE u.account_status = 1
            GROUP BY u.id, u.username, u.employee_id
            HAVING assigned_tasks > 0 OR assigned_subtasks > 0
            ORDER BY assigned_tasks DESC, assigned_subtasks DESC
            LIMIT 100;
        """)
        
        columns = [col[0] for col in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]
```

### 2.5 ç¼“å­˜ç­–ç•¥

#### 2.5.1 å¤šçº§ç¼“å­˜æ¶æ„

```python
# apps/tasks/cache_manager.py
"""
ä»»åŠ¡æ•°æ®ç¼“å­˜ç®¡ç†å™¨
å®ç°å¤šçº§ç¼“å­˜ç­–ç•¥ï¼šæœ¬åœ°å†…å­˜ + Redis
"""

from django.core.cache import cache
from django.conf import settings
import hashlib
import json


class TaskCacheManager:
    """ä»»åŠ¡ç¼“å­˜ç®¡ç†å™¨"""
    
    # ç¼“å­˜é”®å‰ç¼€
    PREFIX_TASK = 'task:'
    PREFIX_TASK_LIST = 'task_list:'
    PREFIX_SUBTASK = 'subtask:'
    PREFIX_SUBTASK_LIST = 'subtask_list:'
    PREFIX_STATS = 'stats:'
    
    # ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
    TTL_TASK_DETAIL = 3600  # 1å°æ—¶
    TTL_TASK_LIST = 300     # 5åˆ†é’Ÿ
    TTL_SUBTASK_DETAIL = 3600
    TTL_SUBTASK_LIST = 600  # 10åˆ†é’Ÿ
    TTL_STATS = 1800        # 30åˆ†é’Ÿ
    
    @classmethod
    def get_task_key(cls, task_id):
        """ç”Ÿæˆä»»åŠ¡ç¼“å­˜é”®"""
        return f"{cls.PREFIX_TASK}{task_id}"
    
    @classmethod
    def get_task_list_key(cls, **filters):
        """ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ç¼“å­˜é”®"""
        filter_str = json.dumps(filters, sort_keys=True)
        hash_key = hashlib.md5(filter_str.encode()).hexdigest()[:8]
        return f"{cls.PREFIX_TASK_LIST}{hash_key}"
    
    @classmethod
    def get_subtask_key(cls, subtask_id):
        """ç”Ÿæˆå­ä»»åŠ¡ç¼“å­˜é”®"""
        return f"{cls.PREFIX_SUBTASK}{subtask_id}"
    
    @classmethod
    def cache_task(cls, task_id, task_data):
        """ç¼“å­˜å•ä¸ªä»»åŠ¡"""
        cache_key = cls.get_task_key(task_id)
        cache.set(cache_key, task_data, timeout=cls.TTL_TASK_DETAIL)
    
    @classmethod
    def get_cached_task(cls, task_id):
        """è·å–ç¼“å­˜çš„ä»»åŠ¡"""
        cache_key = cls.get_task_key(task_id)
        return cache.get(cache_key)
    
    @classmethod
    def invalidate_task(cls, task_id):
        """ä½¿ä»»åŠ¡ç¼“å­˜å¤±æ•ˆ"""
        cache_key = cls.get_task_key(task_id)
        cache.delete(cache_key)
        # åŒæ—¶æ¸…é™¤ç›¸å…³åˆ—è¡¨ç¼“å­˜
        cls.invalidate_task_list_cache()
    
    @classmethod
    def cache_task_list(cls, cache_key, task_list, timeout=None):
        """ç¼“å­˜ä»»åŠ¡åˆ—è¡¨"""
        if timeout is None:
            timeout = cls.TTL_TASK_LIST
        cache.set(cache_key, task_list, timeout=timeout)
    
    @classmethod
    def get_cached_task_list(cls, **filters):
        """è·å–ç¼“å­˜çš„ä»»åŠ¡åˆ—è¡¨"""
        cache_key = cls.get_task_list_key(**filters)
        return cache.get(cache_key)
    
    @classmethod
    def invalidate_task_list_cache(cls):
        """æ¸…é™¤æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ç¼“å­˜"""
        # ä½¿ç”¨é€šé…ç¬¦åˆ é™¤ï¼ˆå¦‚æœåç«¯æ”¯æŒï¼‰
        if hasattr(cache, 'delete_pattern'):
            cache.delete_pattern(f"{cls.PREFIX_TASK_LIST}*")
    
    @classmethod
    def cache_statistics(cls, stats_type, data):
        """ç¼“å­˜ç»Ÿè®¡æ•°æ®"""
        cache_key = f"{cls.PREFIX_STATS}{stats_type}"
        cache.set(cache_key, data, timeout=cls.TTL_STATS)
    
    @classmethod
    def get_cached_statistics(cls, stats_type):
        """è·å–ç¼“å­˜çš„ç»Ÿè®¡æ•°æ®"""
        cache_key = f"{cls.PREFIX_STATS}{stats_type}"
        return cache.get(cache_key)


# è£…é¥°å™¨ï¼šè‡ªåŠ¨ç¼“å­˜æŸ¥è¯¢ç»“æœ
def cached_query(cache_key_func, timeout=300):
    """
    æŸ¥è¯¢ç»“æœç¼“å­˜è£…é¥°å™¨
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    @cached_query(lambda self: f"user_tasks_{self.user_id}", timeout=600)
    def get_user_tasks(self):
        return Task.objects.filter(user_id=self.user_id)
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            cache_key = cache_key_func(*args, **kwargs)
            result = cache.get(cache_key)
            
            if result is None:
                result = func(*args, **kwargs)
                cache.set(cache_key, result, timeout=timeout)
            
            return result
        return wrapper
    return decorator
```

#### 2.5.2 Djangoæ¨¡å‹ä¿¡å·è‡ªåŠ¨æ›´æ–°ç¼“å­˜

```python
# apps/tasks/signals.py
"""
Djangoä¿¡å·ï¼šè‡ªåŠ¨ç®¡ç†ç¼“å­˜
"""

from django.db.models.signals import post_save, post_delete
from django.dispatch import receiver
from .models import TestTask, SubTask
from .cache_manager import TaskCacheManager


@receiver(post_save, sender=TestTask)
def invalidate_task_cache_on_save(sender, instance, **kwargs):
    """ä»»åŠ¡ä¿å­˜æ—¶æ¸…é™¤ç›¸å…³ç¼“å­˜"""
    TaskCacheManager.invalidate_task(instance.id)
    
    # æ¸…é™¤ç”¨æˆ·ç›¸å…³ç¼“å­˜
    if instance.requester_id:
        cache_key = f"user_tasks_{instance.requester_id}_requester"
        cache.delete(cache_key)
    
    if instance.assignee_id:
        cache_key = f"user_tasks_{instance.assignee_id}_assignee"
        cache.delete(cache_key)


@receiver(post_delete, sender=TestTask)
def invalidate_task_cache_on_delete(sender, instance, **kwargs):
    """ä»»åŠ¡åˆ é™¤æ—¶æ¸…é™¤ç¼“å­˜"""
    TaskCacheManager.invalidate_task(instance.id)


@receiver(post_save, sender=SubTask)
def invalidate_subtask_cache_on_save(sender, instance, **kwargs):
    """å­ä»»åŠ¡ä¿å­˜æ—¶æ¸…é™¤ç›¸å…³ç¼“å­˜"""
    cache_key = TaskCacheManager.get_subtask_key(instance.id)
    cache.delete(cache_key)
    
    # æ¸…é™¤çˆ¶ä»»åŠ¡å­ä»»åŠ¡åˆ—è¡¨ç¼“å­˜
    parent_cache_key = f"subtasks_parent_{instance.parent_task_id}"
    cache.delete(parent_cache_key)
    
    # æ¸…é™¤è´Ÿè´£äººç¼“å­˜
    if instance.assignee_id:
        cache_key = f"user_subtasks_{instance.assignee_id}"
        cache.delete(cache_key)
```

### 2.6 æ•°æ®åº“å‚æ•°è°ƒä¼˜

#### 2.6.1 MySQLé…ç½®æ–‡ä»¶ä¼˜åŒ–ï¼ˆmy.cnfï¼‰

```ini
# /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]
# åŸºç¡€é…ç½®
user = mysql
port = 3306
basedir = /usr
datadir = /var/lib/mysql
tmpdir = /tmp

# å­—ç¬¦é›†
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

# InnoDBä¼˜åŒ–ï¼ˆç™¾ä¸‡çº§æ•°æ®ï¼‰
# ç¼“å†²æ± å¤§å°ï¼šå»ºè®®è®¾ç½®ä¸ºç‰©ç†å†…å­˜çš„50-70%
innodb_buffer_pool_size = 4G

# æ—¥å¿—æ–‡ä»¶å¤§å°ï¼ˆå½±å“å†™å…¥æ€§èƒ½ï¼‰
innodb_log_file_size = 512M
innodb_log_files_in_group = 2

# åˆ·æ–°æ—¥å¿—ç­–ç•¥ï¼ˆå¹³è¡¡æ€§èƒ½ä¸æŒä¹…æ€§ï¼‰
innodb_flush_log_at_trx_commit = 2

# æ¯æ¬¡æäº¤åˆ·æ–°å¤šå°‘ä¸ªäº‹åŠ¡ï¼ˆæå‡æ‰¹é‡æ’å…¥æ€§èƒ½ï¼‰
innodb_flush_method = O_DIRECT

# å¹¶å‘çº¿ç¨‹æ•°
innodb_read_io_threads = 8
innodb_write_io_threads = 8
innodb_thread_concurrency = 16

# è‡ªé€‚åº”å“ˆå¸Œç´¢å¼•ï¼ˆé€‚åˆè¯»å¤šå†™å°‘åœºæ™¯ï¼‰
innodb_adaptive_hash_index = ON

# é¡µå¤§å°ï¼ˆé»˜è®¤16KBï¼Œé€šå¸¸ä¸éœ€è¦æ”¹ï¼‰
# innodb_page_size = 16384

# è¿æ¥ä¼˜åŒ–
max_connections = 500
max_connect_errors = 1000
wait_timeout = 600
interactive_timeout = 600

# æŸ¥è¯¢ç¼“å­˜ï¼ˆMySQL 8.0å·²ç§»é™¤ï¼Œä½¿ç”¨åº”ç”¨å±‚ç¼“å­˜ä»£æ›¿ï¼‰
# query_cache_type = 0
# query_cache_size = 0

# ä¸´æ—¶è¡¨ä¼˜åŒ–
tmp_table_size = 256M
max_heap_table_size = 256M

# æ’åºç¼“å†²
sort_buffer_size = 4M
read_buffer_size = 2M
read_rnd_buffer_size = 8M
join_buffer_size = 4M

# è¡¨æ‰“å¼€ç¼“å­˜
table_open_cache = 4000
table_definition_cache = 2000

# æ…¢æŸ¥è¯¢æ—¥å¿—
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2
log_queries_not_using_indexes = 1

# äºŒè¿›åˆ¶æ—¥å¿—ï¼ˆä¸»ä»å¤åˆ¶ç”¨ï¼‰
server-id = 1
log_bin = /var/log/mysql/mysql-bin
binlog_format = ROW
binlog_row_image = FULL
expire_logs_days = 7
max_binlog_size = 500M

# æ€§èƒ½æ¨¡å¼ï¼ˆç”¨äºç›‘æ§ï¼‰
performance_schema = ON
performance_schema_instrument = '%=on'
```

#### 2.6.2 MySQLè¿è¡Œæ—¶ä¼˜åŒ–å‘½ä»¤

```sql
-- æŸ¥çœ‹å½“å‰é…ç½®
SHOW VARIABLES LIKE 'innodb_buffer_pool_size';
SHOW VARIABLES LIKE 'innodb_log_file_size';

-- æŸ¥çœ‹ç¼“å†²æ± ä½¿ç”¨æƒ…å†µ
SHOW ENGINE INNODB STATUS;

-- æŸ¥çœ‹æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç‡ï¼ˆMySQL 8.0ä¸æ”¯æŒï¼‰
-- SHOW STATUS LIKE 'Qcache%';

-- æŸ¥çœ‹è¿æ¥æ•°
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Max_used_connections';

-- æŸ¥çœ‹æ…¢æŸ¥è¯¢æ•°é‡
SHOW STATUS LIKE 'Slow_queries';

-- æŸ¥çœ‹è¡¨é”æƒ…å†µ
SHOW STATUS LIKE 'Table_locks%';

-- å®æ—¶æŸ¥çœ‹æ­£åœ¨æ‰§è¡Œçš„æŸ¥è¯¢
SHOW PROCESSLIST;

-- æŸ¥çœ‹é”ç­‰å¾…
SELECT * FROM information_schema.INNODB_LOCK_WAITS;
SELECT * FROM information_schema.INNODB_LOCKS;
```

### 2.7 è¯»å†™åˆ†ç¦»é…ç½®

#### 2.7.1 Djangoå¤šæ•°æ®åº“é…ç½®

```python
# laboratory_management/settings.py

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'laboratory_test_management',
        'USER': 'lab_user',
        'PASSWORD': 'Ymzlj729928@',
        'HOST': 'localhost',
        'PORT': '3306',
        'OPTIONS': {
            'init_command': "SET sql_mode='STRICT_TRANS_TABLES'",
        },
    },
    'replica': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'laboratory_test_management',
        'USER': 'lab_readonly',
        'PASSWORD': 'readonly_password',
        'HOST': '192.168.1.100',  # ä»åº“åœ°å€
        'PORT': '3306',
        'OPTIONS': {
            'init_command': "SET sql_mode='STRICT_TRANS_TABLES'",
        },
    }
}

# æ•°æ®åº“è·¯ç”±
DATABASE_ROUTERS = ['apps.common.db_router.PrimaryReplicaRouter']
```

#### 2.7.2 æ•°æ®åº“è·¯ç”±å®ç°

```python
# apps/common/db_router.py
"""
ä¸»ä»æ•°æ®åº“è·¯ç”±
å†™æ“ä½œ -> ä¸»åº“
è¯»æ“ä½œ -> ä»åº“
"""

import random


class PrimaryReplicaRouter:
    """ä¸»ä»æ•°æ®åº“è·¯ç”±"""
    
    def db_for_read(self, model, **hints):
        """è¯»æ“ä½œè·¯ç”±åˆ°ä»åº“"""
        # å¯ä»¥æ·»åŠ é€»è¾‘ï¼šç‰¹å®šæŸ¥è¯¢èµ°ä¸»åº“ï¼ˆå¦‚éœ€è¦æœ€æ–°æ•°æ®ï¼‰
        if hasattr(model, '_force_master'):
            return 'default'
        return 'replica'
    
    def db_for_write(self, model, **hints):
        """å†™æ“ä½œè·¯ç”±åˆ°ä¸»åº“"""
        return 'default'
    
    def allow_relation(self, obj1, obj2, **hints):
        """å…è®¸è·¨æ•°æ®åº“å…³è”"""
        return True
    
    def allow_migrate(self, db, app_label, model_name=None, **hints):
        """åªåœ¨ä¸»åº“æ‰§è¡Œè¿ç§»"""
        return db == 'default'


# å¼ºåˆ¶ä½¿ç”¨ä¸»åº“çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
from django.db import transaction

class ForceMasterDB:
    """
    å¼ºåˆ¶ä½¿ç”¨ä¸»åº“è¿›è¡ŒæŸ¥è¯¢
    é€‚ç”¨äºéœ€è¦æœ€æ–°æ•°æ®çš„åœºæ™¯
    """
    def __enter__(self):
        self.original_db_for_read = PrimaryReplicaRouter.db_for_read
        PrimaryReplicaRouter.db_for_read = lambda self, model, **hints: 'default'
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        PrimaryReplicaRouter.db_for_read = self.original_db_for_read


# ä½¿ç”¨ç¤ºä¾‹
# æ™®é€šæŸ¥è¯¢ï¼ˆèµ°ä»åº“ï¼‰
tasks = TestTask.objects.filter(status_id=1)

# å¼ºåˆ¶èµ°ä¸»åº“
with ForceMasterDB():
    latest_task = TestTask.objects.latest('created_at')
```

---

## ğŸ” ä¸‰ã€æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­

### 3.1 æ…¢æŸ¥è¯¢åˆ†æ

```sql
-- å¼€å¯æ…¢æŸ¥è¯¢æ—¥å¿—
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 2;
SET GLOBAL log_queries_not_using_indexes = 'ON';

-- æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT 
    start_time,
    query_time,
    lock_time,
    rows_sent,
    rows_examined,
    sql_text
FROM mysql.slow_log
WHERE start_time > DATE_SUB(NOW(), INTERVAL 1 DAY)
ORDER BY query_time DESC
LIMIT 20;
```

### 3.2 æŸ¥è¯¢æ€§èƒ½åˆ†æ

```sql
-- åˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’
EXPLAIN ANALYZE
SELECT t.*, s.name as status_name
FROM test_tasks t
INNER JOIN task_status s ON t.status_id = s.id
WHERE t.created_at > '2024-01-01'
AND s.code = 'completed'
ORDER BY t.created_at DESC
LIMIT 100;

-- é‡ç‚¹å…³æ³¨ï¼š
-- type: ALLï¼ˆå…¨è¡¨æ‰«æï¼Œå·®ï¼‰-> range/ref/constï¼ˆå¥½ï¼‰
-- key: æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•
-- rows: æ‰«æè¡Œæ•°
-- Extra: Using filesortï¼ˆéœ€è¦ä¼˜åŒ–ï¼‰, Using temporaryï¼ˆéœ€è¦ä¼˜åŒ–ï¼‰
```

### 3.3 Djangoæ€§èƒ½åˆ†æå·¥å…·

```python
# settings.py

# å¼€å¯SQLæ—¥å¿—
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'level': 'DEBUG',
            'class': 'logging.StreamHandler',
        },
    },
    'loggers': {
        'django.db.backends': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
    },
}

# Django Debug Toolbarï¼ˆå¼€å‘ç¯å¢ƒï¼‰
INSTALLED_APPS += ['debug_toolbar']
MIDDLEWARE = ['debug_toolbar.middleware.DebugToolbarMiddleware'] + MIDDLEWARE
INTERNAL_IPS = ['127.0.0.1']
```

### 3.4 æ€§èƒ½ç›‘æ§è„šæœ¬

```python
# scripts/monitor_database_performance.py
"""
æ•°æ®åº“æ€§èƒ½ç›‘æ§è„šæœ¬
å®šæ—¶æ‰§è¡Œï¼Œæ”¶é›†æ€§èƒ½æŒ‡æ ‡
"""

import os
import sys
import django
from datetime import datetime

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'laboratory_management.settings')
sys.path.insert(0, '/opt/laboratory_management')
django.setup()

from django.db import connection
import json


def collect_metrics():
    """æ”¶é›†æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
    metrics = {
        'timestamp': datetime.now().isoformat(),
        'tables': {}
    }
    
    with connection.cursor() as cursor:
        # è¡¨å¤§å°ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                table_name,
                table_rows,
                data_length / 1024 / 1024 as data_size_mb,
                index_length / 1024 / 1024 as index_size_mb
            FROM information_schema.tables
            WHERE table_schema = DATABASE()
            AND table_name IN ('test_tasks', 'sub_tasks', 'test_tasks_archive', 'sub_tasks_archive')
            ORDER BY data_length DESC;
        """)
        
        for row in cursor.fetchall():
            metrics['tables'][row[0]] = {
                'rows': row[1],
                'data_size_mb': round(row[2], 2),
                'index_size_mb': round(row[3], 2)
            }
        
        # ç´¢å¼•ä½¿ç”¨æƒ…å†µ
        cursor.execute("""
            SELECT 
                table_name,
                index_name,
                cardinality
            FROM information_schema.statistics
            WHERE table_schema = DATABASE()
            AND table_name IN ('test_tasks', 'sub_tasks')
            ORDER BY table_name, index_name;
        """)
        
        metrics['indexes'] = []
        for row in cursor.fetchall():
            metrics['indexes'].append({
                'table': row[0],
                'index': row[1],
                'cardinality': row[2]
            })
        
        # æ…¢æŸ¥è¯¢æ•°é‡
        cursor.execute("SHOW STATUS LIKE 'Slow_queries';")
        slow_queries = cursor.fetchone()
        metrics['slow_queries'] = slow_queries[1] if slow_queries else 0
    
    return metrics


def save_metrics(metrics):
    """ä¿å­˜æŒ‡æ ‡åˆ°æ—¥å¿—æ–‡ä»¶"""
    log_file = '/var/log/laboratory_management/db_metrics.json'
    
    # è¯»å–ç°æœ‰æ—¥å¿—
    if os.path.exists(log_file):
        with open(log_file, 'r') as f:
            try:
                history = json.load(f)
            except:
                history = []
    else:
        history = []
    
    # æ·»åŠ æ–°è®°å½•
    history.append(metrics)
    
    # åªä¿ç•™æœ€è¿‘30å¤©çš„è®°å½•
    history = history[-2880:]  # æ¯15åˆ†é’Ÿä¸€æ¬¡ï¼Œ30å¤© = 2880æ¬¡
    
    # ä¿å­˜
    with open(log_file, 'w') as f:
        json.dump(history, f, indent=2)


if __name__ == '__main__':
    print(f"æ”¶é›†æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡ - {datetime.now()}")
    metrics = collect_metrics()
    save_metrics(metrics)
    print(json.dumps(metrics, indent=2))
```

---

## ğŸ“ˆ å››ã€æ€§èƒ½æµ‹è¯•ä¸éªŒè¯

### 4.1 æ•°æ®ç”Ÿæˆè„šæœ¬ï¼ˆç”¨äºæµ‹è¯•ï¼‰

```python
# scripts/generate_test_data.py
"""
ç”Ÿæˆç™¾ä¸‡çº§æµ‹è¯•æ•°æ®
ç”¨äºæ€§èƒ½æµ‹è¯•
"""

import os
import sys
import django
import random
from datetime import datetime, timedelta

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'laboratory_management.settings')
sys.path.insert(0, '/opt/laboratory_management')
django.setup()

from django.db import transaction
from apps.tasks.models import TestTask, SubTask, TestType, TaskStatus, PriorityType
from apps.users.models import User


def generate_tasks(count=1000000, batch_size=5000):
    """ç”ŸæˆæŒ‡å®šæ•°é‡çš„æµ‹è¯•ä»»åŠ¡"""
    
    # è·å–å¿…è¦çš„å¤–é”®
    test_types = list(TestType.objects.all())
    statuses = list(TaskStatus.objects.all())
    priorities = list(PriorityType.objects.all())
    users = list(User.objects.all())
    
    if not all([test_types, statuses, priorities, users]):
        print("é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„åŸºç¡€æ•°æ®ï¼ˆè¯•éªŒç±»å‹ã€çŠ¶æ€ã€ä¼˜å…ˆçº§æˆ–ç”¨æˆ·ï¼‰")
        return
    
    print(f"å¼€å§‹ç”Ÿæˆ {count} ä¸ªä»»åŠ¡...")
    
    generated = 0
    while generated < count:
        batch = []
        for i in range(min(batch_size, count - generated)):
            task_num = f"TASK{datetime.now().strftime('%Y%m%d')}{generated + i:06d}"
            created_at = datetime.now() - timedelta(days=random.randint(0, 730))
            
            task = TestTask(
                task_number=task_num,
                task_name=f"æµ‹è¯•ä»»åŠ¡ {generated + i}",
                product_name=f"äº§å“{random.randint(1, 100)}",
                product_model=f"MODEL-{random.randint(1000, 9999)}",
                test_type=random.choice(test_types),
                priority=random.choice(priorities),
                status=random.choice(statuses),
                requester=random.choice(users),
                assignee=random.choice(users) if random.random() > 0.3 else None,
                start_date=created_at.date(),
                end_date=(created_at + timedelta(days=random.randint(7, 90))).date(),
                created_at=created_at,
                updated_at=created_at
            )
            batch.append(task)
        
        with transaction.atomic():
            TestTask.objects.bulk_create(batch, batch_size=batch_size)
        
        generated += len(batch)
        print(f"å·²ç”Ÿæˆ: {generated}/{count}")
    
    print(f"å®Œæˆï¼å…±ç”Ÿæˆ {generated} ä¸ªä»»åŠ¡")


def generate_subtasks(task_count=1000000, subtasks_per_task=3):
    """ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå­ä»»åŠ¡"""
    
    test_types = list(TestType.objects.all())
    statuses = list(TaskStatus.objects.all())
    users = list(User.objects.all())
    
    print(f"å¼€å§‹ç”Ÿæˆå­ä»»åŠ¡...")
    
    batch_size = 5000
    generated = 0
    total = task_count * subtasks_per_task
    
    for task in TestTask.objects.all().iterator():
        for i in range(subtasks_per_task):
            subtask_num = f"{task.task_number}-SUB{i+1:02d}"
            
            subtask = SubTask(
                parent_task=task,
                subtask_number=subtask_num,
                subtask_name=f"{task.task_name} - å­ä»»åŠ¡{i+1}",
                test_type=random.choice(test_types),
                status=random.choice(statuses),
                assignee=random.choice(users) if random.random() > 0.3 else None,
                start_date=task.start_date,
                end_date=task.end_date,
                created_at=task.created_at,
                updated_at=task.created_at
            )
            
            # æ‰¹é‡ä¿å­˜
            if len(batch) >= batch_size:
                with transaction.atomic():
                    SubTask.objects.bulk_create(batch)
                generated += len(batch)
                print(f"å·²ç”Ÿæˆ: {generated}/{total}")
                batch = []
    
    if batch:
        with transaction.atomic():
            SubTask.objects.bulk_create(batch)
        generated += len(batch)
    
    print(f"å®Œæˆï¼å…±ç”Ÿæˆ {generated} ä¸ªå­ä»»åŠ¡")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæµ‹è¯•æ•°æ®')
    parser.add_argument('--tasks', type=int, default=100000, help='ä»»åŠ¡æ•°é‡')
    parser.add_argument('--subtasks', type=int, default=3, help='æ¯ä¸ªä»»åŠ¡çš„å­ä»»åŠ¡æ•°')
    
    args = parser.parse_args()
    
    generate_tasks(args.tasks)
    generate_subtasks(args.tasks, args.subtasks)
```

### 4.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# scripts/benchmark_queries.py
"""
æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import os
import sys
import django
import time
from statistics import mean, median

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'laboratory_management.settings')
sys.path.insert(0, '/opt/laboratory_management')
django.setup()

from apps.tasks.models import TestTask, SubTask
from django.db import connection


def benchmark_query(name, query_func, iterations=10):
    """åŸºå‡†æµ‹è¯•å•ä¸ªæŸ¥è¯¢"""
    times = []
    
    for _ in range(iterations):
        start = time.time()
        result = query_func()
        # å¼ºåˆ¶æŸ¥è¯¢æ‰§è¡Œ
        list(result) if hasattr(result, '__iter__') else result
        end = time.time()
        times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
    
    print(f"\n{name}:")
    print(f"  å¹³å‡: {mean(times):.2f}ms")
    print(f"  ä¸­ä½æ•°: {median(times):.2f}ms")
    print(f"  æœ€å°: {min(times):.2f}ms")
    print(f"  æœ€å¤§: {max(times):.2f}ms")


def run_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    
    print("=" * 60)
    print("æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1ï¼šæŒ‰çŠ¶æ€æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨
    benchmark_query(
        "æŒ‰çŠ¶æ€æŸ¥è¯¢ä»»åŠ¡ï¼ˆå‰100æ¡ï¼‰",
        lambda: TestTask.objects.filter(status_id=1).select_related('status')[:100]
    )
    
    # æµ‹è¯•2ï¼šæŒ‰ç”¨æˆ·æŸ¥è¯¢ä»»åŠ¡
    benchmark_query(
        "æŒ‰ç”³è¯·äººæŸ¥è¯¢ä»»åŠ¡",
        lambda: TestTask.objects.filter(requester_id=1).select_related('status', 'test_type')[:50]
    )
    
    # æµ‹è¯•3ï¼šæŸ¥è¯¢ä¸»ä»»åŠ¡ä¸‹çš„å­ä»»åŠ¡
    parent_task_id = TestTask.objects.first().id if TestTask.objects.exists() else 1
    benchmark_query(
        f"æŸ¥è¯¢ä¸»ä»»åŠ¡(ID={parent_task_id})ä¸‹çš„å­ä»»åŠ¡",
        lambda: SubTask.objects.filter(parent_task_id=parent_task_id).select_related('status')[:20]
    )
    
    # æµ‹è¯•4ï¼šé€¾æœŸä»»åŠ¡æŸ¥è¯¢
    from django.utils import timezone
    today = timezone.now().date()
    benchmark_query(
        "æŸ¥è¯¢é€¾æœŸä»»åŠ¡",
        lambda: TestTask.objects.filter(end_date__lt=today, status_id=1).select_related('assignee')[:50]
    )
    
    # æµ‹è¯•5ï¼šä»»åŠ¡ç»Ÿè®¡
    benchmark_query(
        "ä»»åŠ¡ç»Ÿè®¡æŸ¥è¯¢",
        lambda: TestTask.objects.values('status_id').annotate(count=models.Count('id'))
    )
    
    print("\n" + "=" * 60)
    print("åŸºå‡†æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == '__main__':
    run_benchmarks()
```

---

## âœ… äº”ã€å®æ–½æ¸…å•

### 5.1 ç«‹å³æ‰§è¡Œï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

- [ ] 1. æ‰§è¡Œç´¢å¼•ä¼˜åŒ–SQLï¼ˆ2.1èŠ‚ï¼‰
- [ ] 2. åˆ›å»ºDjangoè¿ç§»æ–‡ä»¶ï¼ˆ2.1.3èŠ‚ï¼‰
- [ ] 3. é…ç½®MySQLå‚æ•°ï¼ˆ2.6.1èŠ‚ï¼‰
- [ ] 4. éƒ¨ç½²ç¼“å­˜ç®¡ç†å™¨ï¼ˆ2.5èŠ‚ï¼‰
- [ ] 5. é…ç½®Djangoä¿¡å·ï¼ˆ2.5.2èŠ‚ï¼‰

### 5.2 çŸ­æœŸæ‰§è¡Œï¼ˆ1-2å‘¨å†…ï¼‰

- [ ] 6. åˆ›å»ºå½’æ¡£è¡¨å’Œå½’æ¡£è„šæœ¬ï¼ˆ2.3èŠ‚ï¼‰
- [ ] 7. é…ç½®å®šæ—¶ä»»åŠ¡ï¼ˆcrontabï¼‰
- [ ] 8. ä¼˜åŒ–DjangoæŸ¥è¯¢ä»£ç ï¼ˆ2.4èŠ‚ï¼‰
- [ ] 9. éƒ¨ç½²æ€§èƒ½ç›‘æ§è„šæœ¬ï¼ˆ3.4èŠ‚ï¼‰
- [ ] 10. æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆ4.2èŠ‚ï¼‰

### 5.3 ä¸­æœŸæ‰§è¡Œï¼ˆ1ä¸ªæœˆå†…ï¼‰

- [ ] 11. å®æ–½è¡¨åˆ†åŒºç­–ç•¥ï¼ˆ2.2èŠ‚ï¼‰
- [ ] 12. é…ç½®è¯»å†™åˆ†ç¦»ï¼ˆ2.7èŠ‚ï¼‰
- [ ] 13. ç”Ÿæˆæµ‹è¯•æ•°æ®å¹¶éªŒè¯ï¼ˆ4.1èŠ‚ï¼‰
- [ ] 14. å»ºç«‹æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
- [ ] 15. ç¼–å†™æ€§èƒ½ä¼˜åŒ–æ–‡æ¡£

### 5.4 éªŒæ”¶æ ‡å‡†

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–åç›®æ ‡ | éªŒè¯æ–¹æ³• |
|------|--------|-----------|----------|
| ä»»åŠ¡åˆ—è¡¨æŸ¥è¯¢ | > 2ç§’ | < 500ms | åŸºå‡†æµ‹è¯• |
| ç”¨æˆ·ä»»åŠ¡æŸ¥è¯¢ | > 1.5ç§’ | < 300ms | åŸºå‡†æµ‹è¯• |
| å­ä»»åŠ¡æŸ¥è¯¢ | > 1ç§’ | < 200ms | åŸºå‡†æµ‹è¯• |
| æ•°æ®åº“CPUä½¿ç”¨ç‡ | > 80% | < 50% | ç›‘æ§å·¥å…· |
| å†…å­˜ä½¿ç”¨ | ä¸ç¨³å®š | ç¨³å®šåœ¨4GBå†… | ç›‘æ§å·¥å…· |
| æ…¢æŸ¥è¯¢æ•°é‡ | 100+/å¤© | < 10/å¤© | æ…¢æŸ¥è¯¢æ—¥å¿— |
| å¹¶å‘è¿æ¥æ•° | ç»å¸¸è€—å°½ | < 200 | çŠ¶æ€ç›‘æ§ |

---

## ğŸ“š å…­ã€å‚è€ƒèµ„æº

### 6.1 MySQLå®˜æ–¹æ–‡æ¡£
- [MySQLç´¢å¼•ä¼˜åŒ–](https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html)
- [è¡¨åˆ†åŒº](https://dev.mysql.com/doc/refman/8.0/en/partitioning.html)
- [InnoDBæ€§èƒ½ä¼˜åŒ–](https://dev.mysql.com/doc/refman/8.0/en/optimizing-innodb.html)

### 6.2 Djangoæ–‡æ¡£
- [æ•°æ®åº“ä¼˜åŒ–](https://docs.djangoproject.com/en/4.2/topics/db/optimization/)
- [ç¼“å­˜æ¡†æ¶](https://docs.djangoproject.com/en/4.2/topics/cache/)
- [å¤šæ•°æ®åº“é…ç½®](https://docs.djangoproject.com/en/4.2/topics/db/multi-db/)

### 6.3 æ€§èƒ½æµ‹è¯•å·¥å…·
- `mysqlslap` - MySQLè‡ªå¸¦çš„å‹åŠ›æµ‹è¯•å·¥å…·
- `sysbench` - å¼€æºæ•°æ®åº“åŸºå‡†æµ‹è¯•å·¥å…·
- `pt-query-digest` - Percona Toolkitæ…¢æŸ¥è¯¢åˆ†æ

---

**æ–‡æ¡£ç»´æŠ¤ï¼š**
- å»ºè®®æ¯æœˆreviewä¸€æ¬¡æ…¢æŸ¥è¯¢æ—¥å¿—
- æ¯å­£åº¦è¯„ä¼°ç´¢å¼•ä½¿ç”¨æƒ…å†µ
- æ•°æ®é‡è¾¾åˆ°500ä¸‡æ—¶è€ƒè™‘è¿›ä¸€æ­¥åˆ†åº“åˆ†è¡¨

**è”ç³»æ–¹å¼ï¼š**
- æŠ€æœ¯è´Ÿè´£äººï¼š[å¾…å¡«å†™]
- DBAï¼š[å¾…å¡«å†™]
- è¿ç»´å›¢é˜Ÿï¼š[å¾…å¡«å†™]
